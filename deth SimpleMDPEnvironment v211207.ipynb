{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Agent and Environment\n",
    "\n",
    "This example of a rational agent enables experimentation with decision strategies based on expected utility. \n",
    "\n",
    "The environment interface is inspired by the Open AI gym framework. \n",
    "\n",
    "The example from chapter 16 of AIMA (4th ed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Definition of the Environment\n",
    "\n",
    "The code below defines all characteristics of the Markov Decision Process of the AIMA book in chapter 16 with the following characteristics:\n",
    "\n",
    "- States: The agent is in one of the following positions {(1, 1), (2, 1), (3, 1), (4, 1), (1, 2), (3, 2), (4, 2), (1, 3), (2, 3), (3, 3), (4, 3)}.\n",
    "- Actions: The agent can choose between {Up, Down, Left, Right}\n",
    "- Transitions: There is a probability of 0.8/0.1/0.1 that action Up results in an upward/left/right movement, provided that the state exists. Movements 'outside the world' or to (2, 2) result in the state remaining the same. \n",
    "\n",
    "An SimpleMDPEnvironment object has the following methods:\n",
    "- reset() which brings the environment the start state, which is also returned\n",
    "- step(action) processes the action of the agent and returns the new state, done, reward (and optional debug info)\n",
    "- render() simple visualisation of the current state of the world\n",
    "\n",
    "To allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "\n",
    "- get_possible_states() for iterating over all possible states\n",
    "- is_done(state) for excluding the stop states from the policy\n",
    "- get_reward(state) simplified version $R(s)$ of the general reward function: $R(s, a, s')$\n",
    "- get_transition_prob(action, new_state, old_state): $P(s' \\mid s, a)$\n",
    "\n",
    "We will illustrate each of the elements above by simple code examples below.  \n",
    "All of the theory can be found in AIMA 16.1 and 16.2.1 (4th ed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from random import randint, choice\n",
    "from copy import copy\n",
    "\n",
    "Action = Enum('Action', 'Left Right Up Down')\n",
    "    \n",
    "Drift = Enum('Drift', 'Left Not Right')    \n",
    "\n",
    "class SimpleMDPEnvironment():\n",
    "    def __init__(self, initial_state, reward_per_step):\n",
    "        self.__x_min, self.__y_min = (1, 1)\n",
    "        self.__x_max, self.__y_max = (4, 3)\n",
    "        self.__possible_states = \\\n",
    "        [(1, 1), (2, 1), (3, 1), (4, 1), \n",
    "         (1, 2), (3, 2), (4, 2), (1, 3), \n",
    "         (2, 3), (3, 3), (4, 3)]\n",
    "        if initial_state in self.__possible_states:\n",
    "            self.__initial_state = initial_state\n",
    "        else:\n",
    "            self.__initial_state = (1, 1)\n",
    "        self.__state = self.__initial_state\n",
    "        self.__reward_per_step = reward_per_step\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__state = self.__initial_state\n",
    "        return self.__state\n",
    "\n",
    "    def __calculate_transition(self, action):\n",
    "        # helper method for public method step(self, action)\n",
    "        # determine drift (10% to the left, 80% no drift, 10% to the right)\n",
    "        n = randint(1, 10)\n",
    "        if n == 1:\n",
    "            drift = Drift.Left\n",
    "        elif n == 10:\n",
    "            drift = Drift.Right\n",
    "        else:  # 1 < n < 10\n",
    "            drift = Drift.Not\n",
    "        # determine position\n",
    "        x_old, y_old = self.__state\n",
    "        if action == Action.Up:\n",
    "            if drift == Drift.Left:\n",
    "                x_new, y_new = x_old - 1, y_old\n",
    "            elif drift == Drift.Not:\n",
    "                x_new, y_new = x_old, y_old + 1\n",
    "            elif drift == Drift.Right:\n",
    "                x_new, y_new = x_old + 1, y_old\n",
    "        if action == Action.Down:\n",
    "            if drift == Drift.Left:\n",
    "                x_new, y_new = x_old + 1, y_old\n",
    "            elif drift == Drift.Not:\n",
    "                x_new, y_new = x_old, y_old - 1\n",
    "            elif drift == Drift.Right:\n",
    "                x_new, y_new = x_old - 1, y_old\n",
    "        if action == Action.Left:\n",
    "            if drift == Drift.Left:\n",
    "                x_new, y_new = x_old, y_old - 1\n",
    "            elif drift == Drift.Not:\n",
    "                x_new, y_new = x_old - 1, y_old\n",
    "            elif drift == Drift.Right:\n",
    "                x_new, y_new = x_old, y_old + 1\n",
    "        if action == Action.Right:\n",
    "            if drift == Drift.Left:\n",
    "                x_new, y_new = x_old, y_old + 1\n",
    "            elif drift == Drift.Not:\n",
    "                x_new, y_new = x_old + 1, y_old\n",
    "            elif drift == Drift.Right:\n",
    "                x_new, y_new = x_old, y_old - 1\n",
    "        \n",
    "        if (x_new, y_new) in self.__possible_states:\n",
    "            new_state = (x_new, y_new)\n",
    "        else:\n",
    "            new_state = self.__state  # state does not change\n",
    "            \n",
    "        return new_state\n",
    "      \n",
    "    def step(self, action):\n",
    "        old_state = self.__state\n",
    "        self.__state = self.__calculate_transition(action)  # state after action\n",
    "        observation = self.__state  # environment is fully observable\n",
    "        done = self.is_done()\n",
    "        reward = self.get_reward(self.__state)\n",
    "        info = {}  # optional debug info\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    def render(self):\n",
    "        BACKGROUND = [\n",
    "            '  ┌───────┬───────┬───────┬───────┐',\n",
    "            '  │       │       │       │       │',\n",
    "            '3 │       │       │       │  + 1  │',\n",
    "            '  │       │       │       │       │',\n",
    "            '  ├───────┼───────┼───────┼───────┤',\n",
    "            '  │       │░░░░░░░│       │       │',\n",
    "            '2 │       │░░░░░░░│       │  - 1  │',\n",
    "            '  │       │░░░░░░░│       │       │',\n",
    "            '  ├───────┼───────┼───────┼───────┤',\n",
    "            '  │       │       │       │       │',\n",
    "            '1 │       │       │       │       │',\n",
    "            '  │       │       │       │       │',\n",
    "            '  └───────┴───────┴───────┴───────┘',\n",
    "            '      1       2       3       4'\n",
    "        ]\n",
    "        cell_width, cell_height, left_x, bottom_y = 8, 4, -2, 15\n",
    "        rendering = copy(BACKGROUND)\n",
    "        # insert '*' at the current state\n",
    "        x, y = self.__state\n",
    "        line_nr = bottom_y - cell_height * y\n",
    "        char_nr = left_x + cell_width * x\n",
    "        old_line = rendering[line_nr]\n",
    "        rendering[line_nr] = old_line[:char_nr] + '*' + old_line[char_nr+1:]\n",
    "        \n",
    "        for line in rendering:\n",
    "            print(line)\n",
    "        print('reward per step:', self.__reward_per_step)    \n",
    "\n",
    "    #=========================================================\n",
    "    # public functions for agent to calculate optimal policy\n",
    "    #=========================================================\n",
    "    \n",
    "    def get_possible_states(self):\n",
    "        return self.__possible_states\n",
    "    \n",
    "    def is_done(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        return (state == (4, 2) or state == (4, 3))\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        # Reward R(s) for every possible state\n",
    "        if state == (4, 3):\n",
    "            return 1.0\n",
    "        if state == (4, 2):\n",
    "            return -1.0\n",
    "        return self.__reward_per_step\n",
    "        \n",
    "    def get_transition_prob(self, action, new_state, old_state=None):\n",
    "        if old_state is None:\n",
    "            old_state = self.__state\n",
    "        # returns the Transition Probability P(s'| s, a)\n",
    "        # with s = old_state, a = action and s' = new_state\n",
    "\n",
    "        # distinction in 4 cases\n",
    "        \n",
    "        # case 1: if state is a stop state -> Prob = 0.0\n",
    "        if self.is_done(old_state):\n",
    "            return 0.0\n",
    "        \n",
    "        # case 2: if new_state not possible -> Prob = 0.0\n",
    "        # examples of impossible new_state: (2, 2), (1, 0)\n",
    "        if new_state not in self.__possible_states:\n",
    "            return 0.0\n",
    "\n",
    "        # case 3: if new state != old_state and movement not blocked by a 'wall'\n",
    "        x_old, y_old = old_state\n",
    "        x_new, y_new = new_state\n",
    "        if action == Action.Up:\n",
    "            (dx_s, dy_s), (dx_l, dy_l), (dx_r, dy_r) = ( 0,  1), (-1,  0), ( 1,  0)\n",
    "        elif action == Action.Down:\n",
    "            (dx_s, dy_s), (dx_l, dy_l), (dx_r, dy_r) = ( 0, -1), ( 1,  0), (-1,  0)\n",
    "        elif action == Action.Left:\n",
    "            (dx_s, dy_s), (dx_l, dy_l), (dx_r, dy_r) = (-1,  0), ( 0, -1), ( 0,  1)\n",
    "        else:  # action == Action.Right:\n",
    "            (dx_s, dy_s), (dx_l, dy_l), (dx_r, dy_r) = ( 1,  0), ( 0,  1), ( 0, -1)\n",
    "\n",
    "        (x_no_drift,    y_no_drift)    = (x_old + dx_s, y_old + dy_s)\n",
    "        (x_drift_left,  y_drift_left)  = (x_old + dx_l, y_old + dy_l)\n",
    "        (x_drift_right, y_drift_right) = (x_old + dx_r, y_old + dy_r)\n",
    "\n",
    "        if (x_new, y_new) == (x_no_drift, y_no_drift):\n",
    "            return 0.8\n",
    "        if (x_new, y_new) == (x_drift_left, y_drift_left):\n",
    "            return 0.1\n",
    "        if (x_new, y_new) == (x_drift_right, y_drift_right):\n",
    "            return 0.1\n",
    "        if (x_new, y_new) != (x_old, y_old):\n",
    "            return 0.0  # all other cases: Prob = 0.0 (e.g. states are no neighbors)\n",
    "        \n",
    "        # case 4: new_state = old_state and movement blocked by one or more 'walls'\n",
    "        prob = 0.0\n",
    "        if (x_no_drift, y_no_drift) not in self.__possible_states:\n",
    "            prob += 0.8\n",
    "        if (x_drift_left, y_drift_left) not in self.__possible_states:\n",
    "            prob += 0.1\n",
    "        if (x_drift_right, y_drift_right) not in self.__possible_states:\n",
    "            prob += 0.1\n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of an Environment\n",
    "\n",
    "The Environment Class allows creation of an Environment with an initial state as parameter s = (1, 1).\n",
    "Also, method reset() will set the state back to (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ┌───────┬───────┬───────┬───────┐\n",
      "  │       │       │       │       │\n",
      "3 │       │       │       │  + 1  │\n",
      "  │       │       │       │       │\n",
      "  ├───────┼───────┼───────┼───────┤\n",
      "  │       │░░░░░░░│       │       │\n",
      "2 │       │░░░░░░░│       │  - 1  │\n",
      "  │       │░░░░░░░│       │       │\n",
      "  ├───────┼───────┼───────┼───────┤\n",
      "  │       │       │       │       │\n",
      "1 │       │       │       │       │\n",
      "  │   *   │       │       │       │\n",
      "  └───────┴───────┴───────┴───────┘\n",
      "      1       2       3       4\n",
      "reward per step: -0.04\n",
      "  ┌───────┬───────┬───────┬───────┐\n",
      "  │       │       │       │       │\n",
      "3 │       │       │       │  + 1  │\n",
      "  │       │       │       │       │\n",
      "  ├───────┼───────┼───────┼───────┤\n",
      "  │       │░░░░░░░│       │       │\n",
      "2 │       │░░░░░░░│       │  - 1  │\n",
      "  │       │░░░░░░░│       │       │\n",
      "  ├───────┼───────┼───────┼───────┤\n",
      "  │       │       │       │       │\n",
      "1 │       │       │       │       │\n",
      "  │   *   │       │       │       │\n",
      "  └───────┴───────┴───────┴───────┘\n",
      "      1       2       3       4\n",
      "reward per step: -0.04\n"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in the default state\n",
    "mdp = SimpleMDPEnvironment(initial_state=(1, 1), reward_per_step=-0.04)\n",
    "mdp.get_possible_states()\n",
    "mdp.reset()\n",
    "mdp.render()\n",
    "mdp.step(Action.Up)\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "We will only deal with environments with a finite number of discrete actions.\n",
    "\n",
    "In that case the so-called Action Space (set of all possible actions) can be easily represented by an enum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 1 is Action.Left\n",
      "action 2 is Action.Right\n",
      "action 3 is Action.Up\n",
      "action 4 is Action.Down\n"
     ]
    }
   ],
   "source": [
    "for nr, action in enumerate(Action, 1):\n",
    "    print('action', nr, 'is', action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitions\n",
    "\n",
    "Transitions can be done by calling method step(action).\n",
    "\n",
    "Since the environment is stochastic, the new state may vary according to these rules:\n",
    " - there is a 80% probability that the action is carried out without 'drift'\n",
    " - there is a 20% probability that the movement has a 'drift to the left'  \n",
    " (drift to the left means: Left instead of Up, Up instead of Right, Right instead of Down or Down instead of Left)\n",
    " - there is a 20% probability that the movement has a 'drift to the right'  \n",
    " (drift to the right means: Right instead of Up, Down instead of Right, Left instead of Down or Up instead of Left)\n",
    "If the new position would be outside the environment ('through a wall') the new state is identical to the old state\n",
    "\n",
    "Here is an 'experiment' in which the action Up is repeated on an environment in state (3,1). All possible stats $s'$ with a transition probability T((3, 1), Up, $s'$) are reported with the observed relative frequencies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) -> Up -> (2, 1) has probability: 0.1011\n",
      "(3, 1) -> Up -> (3, 2) has probability: 0.79891\n",
      "(3, 1) -> Up -> (4, 1) has probability: 0.09999\n"
     ]
    }
   ],
   "source": [
    "new_states = {}\n",
    "mdp = SimpleMDPEnvironment(initial_state = (3, 1), reward_per_step=-0.04)\n",
    "N = 100000\n",
    "for _ in range(N):\n",
    "    mdp.reset()\n",
    "    new_state, done, reward, info = mdp.step(Action.Up)\n",
    "    if new_state not in new_states:\n",
    "        new_states[new_state] = 1\n",
    "    else:\n",
    "        new_states[new_state] = new_states[new_state] + 1\n",
    "for new_state in new_states:\n",
    "    print('(3, 1) -> Up ->', new_state, 'has probability:', new_states[new_state] / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition probability $P(s' \\mid s, a)$ can also be returned directly via method get_transition_prob(action, new_state, old_state). This means that the agent has information about the environment model. N.B. this is not always the case in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) -> Up   -> (3, 2) has probability: 0.8\n",
      "(3, 1) -> Up   -> (2, 1) has probability: 0.1\n",
      "(3, 1) -> Up   -> (4, 1) has probability: 0.1\n",
      "(3, 1) -> Up   -> (1, 1) has probability: 0.0\n",
      "(3, 1) -> Up   -> (3, 1) has probability: 0.0\n",
      "(1, 3) -> Left -> (1, 2) has probability: 0.1\n",
      "(1, 3) -> Left -> (1, 3) has probability: 0.9\n",
      "(1, 3) -> Left -> (2, 3) has probability: 0.0\n"
     ]
    }
   ],
   "source": [
    "# check with no walls blocking the movement\n",
    "mdp = SimpleMDPEnvironment(initial_state = (3, 1), reward_per_step=-0.04)\n",
    "for new_state in [(3,2), (2,1), (4,1), (1, 1), (3, 1)]:\n",
    "    print('(3, 1) -> Up   ->', new_state, 'has probability:', mdp.get_transition_prob(Action.Up, new_state))\n",
    "\n",
    "# check in the top left corner\n",
    "mdp = SimpleMDPEnvironment(initial_state = (1, 3), reward_per_step=-0.04)\n",
    "for new_state in [(1,2), (1,3), (2,3)]:\n",
    "    print('(1, 3) -> Left ->', new_state, 'has probability:', mdp.get_transition_prob(Action.Left, new_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Agent\n",
    "\n",
    "The policy function $\\pi(s) \\to a$ is the concrete implementation of the decision process of the agent (selection of an action $a$). In the cell below, you can see the effect of an agent with a random policy choosing an arbitrary action regardless of the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: (1, 1)\n",
      "action: Action.Up\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 3), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 3), reward: -0.04\n",
      "action: Action.Right\tstate: (1, 3), reward: -0.04\n",
      "action: Action.Right\tstate: (2, 3), reward: -0.04\n",
      "action: Action.Down\tstate: (2, 3), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 3), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 3), reward: -0.04\n",
      "action: Action.Right\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 1), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Left\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 1), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Right\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Left\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Right\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 1), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Down\tstate: (1, 1), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 2), reward: -0.04\n",
      "action: Action.Up\tstate: (1, 3), reward: -0.04\n",
      "action: Action.Right\tstate: (2, 3), reward: -0.04\n",
      "action: Action.Right\tstate: (3, 3), reward: -0.04\n",
      "action: Action.Down\tstate: (3, 2), reward: -0.04\n",
      "action: Action.Left\tstate: (3, 2), reward: -0.04\n",
      "action: Action.Right\tstate: (4, 2), reward: -1.00\n",
      "Episode done after 30 steps. total reward:  -2.16\n"
     ]
    }
   ],
   "source": [
    "def policy_random(state):\n",
    "    # action is random choice from all actions in Action Space\n",
    "    action = choice([a for a in Action])\n",
    "    return action\n",
    "\n",
    "# create a random environment\n",
    "mdp = SimpleMDPEnvironment(initial_state=(1, 1), reward_per_step=-0.04)\n",
    "state = mdp.reset()\n",
    "print('initial state: {}'.format(state))\n",
    "\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "nr_steps = 0\n",
    "while not done:\n",
    "    next_action = policy_random(state)\n",
    "    state, done, reward, info = mdp.step(next_action)\n",
    "    total_reward += reward\n",
    "    nr_steps += 1\n",
    "    print('action: {}\\tstate: {}, reward: {:5.2f}'\n",
    "          .format(next_action, state, reward))\n",
    "print('Episode done after {} steps. total reward: {:6.2f}'.format(nr_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell above a number of times, you can observe two things:\n",
    "- The environment always ends up in state (4, 2) or state (4, 3), this is by definition of the stop criterium (done). \n",
    "- The total reward will vary from run to run.\n",
    "\n",
    "Each run from start state until stop state is called an episode.  \n",
    "Let's assemble some statistics on the episodes of the random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 100 episodes\n",
      "mean:  -1.75, sigma:   1.59\n",
      "\n",
      "ep:  1, total reward: -2.44\n",
      "ep:  2, total reward:  0.12\n",
      "ep:  3, total reward: -1.20\n",
      "ep:  4, total reward: -4.48\n",
      "ep:  5, total reward: -2.40\n",
      "......\n",
      "ep: 95, total reward: -2.68\n",
      "ep: 96, total reward: -2.16\n",
      "ep: 97, total reward: -1.28\n",
      "ep: 98, total reward:  0.28\n",
      "ep: 99, total reward: -1.56\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def run_one_episode(policy):\n",
    "    mdp = SimpleMDPEnvironment(initial_state=(1, 1), reward_per_step=-0.04)\n",
    "    state = mdp.reset()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_action = policy(state)\n",
    "        state, done, reward, info = mdp.step(next_action)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def measure_performance(policy, nr_episodes=100):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for _ in range(N):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print('mean: {:6.2f}, sigma: {:6.2f}'.format(mean(all_rewards), stdev(all_rewards)))\n",
    "    print()\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "    print('......')\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards)-5):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "\n",
    "measure_performance(policy_random)  # in Python a function pointer is simply the name of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above multiple times gives an idea of the typical performance (total reward) of the random policy on this environment (start=(1,1), reward_per_step=-0.04). We get a consistent result if we average over enough episodes. \n",
    "\n",
    "Let's see what is the effect of a simple fixed policy: Always choose action Up.\n",
    "\n",
    "Running the same statistics on function policy_up we see that the the total reward also varies between episodes, this is due to the stochastic nature of the environment. The same action does not always yield the same transition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 5000 episodes\n",
      "mean:  -1.43, sigma:   2.00\n",
      "\n",
      "ep:  1, total reward: -3.32\n",
      "ep:  2, total reward: -15.96\n",
      "ep:  3, total reward: -1.96\n",
      "ep:  4, total reward: -0.20\n",
      "ep:  5, total reward: -3.92\n",
      "......\n",
      "ep: 4995, total reward:  0.56\n",
      "ep: 4996, total reward:  0.64\n",
      "ep: 4997, total reward: -3.84\n",
      "ep: 4998, total reward:  0.64\n",
      "ep: 4999, total reward: -2.04\n"
     ]
    }
   ],
   "source": [
    "def policy_only_up(state):\n",
    "    return Action.Up\n",
    "\n",
    "measure_performance(policy_only_up, nr_episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimal decisions based on sums of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman showed in 1957 that the optimal policy $\\pi^{*}(s)$ for an MDP is:\n",
    "\n",
    "(1) $\\pi^{*}(s) = \\underset{a}{argmax} \\space \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$,\n",
    "\n",
    "provided that utility function U(s) satisfies Bellman's equation:\n",
    "\n",
    "(2) $U(s) = \\underset{a}{max} \\space \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$.\n",
    "\n",
    "One can show that Bellman's equation can always be solved and with a single solution.\n",
    "\n",
    "It is useful to define the so-called Q-function:\n",
    "\n",
    "(3) $Q(s, a) = \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$\n",
    "\n",
    "Which simplifies equations (1) and (2) to:\n",
    "\n",
    "(4) $\\pi^{*}(s) = \\underset{a}{argmax} \\space Q(s, a)$  \n",
    "and\n",
    "\n",
    "(5) $U(s) = \\underset{a}{max} \\space Q(s, a)$\n",
    "\n",
    "Thus, finding the optimal policy is reduced to solving Bellman's equation. There are several strategies for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solving the Bellman Equation: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is based on the Bellman update:\n",
    "\n",
    "(6) $U_{i+1}(s) = \\underset{a}{max} \\sum_{s'} P(s' \\mid s, a) \\space [ R(s, a, s') + \\gamma \\space U_i(s') ]$\n",
    "\n",
    "Using equation (3) this simplifies to:\n",
    "\n",
    "(7) $U_{i+1}(s) = \\underset{a}{max} \\space Q_i(s, a)$\n",
    "\n",
    "One can prove that after enough iterations $U_{i+1}(s) \\approx U(s)$, after which Bellman's equation is satisfied.  \n",
    "Since there is only one solution to Bellman's equation, it does not matter with which $U_0(s)$ you start!\n",
    "\n",
    "The algorithm below is Value Iteration with one simplification: $\\gamma$ the so-called discount factor, is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities:\n",
      "   (1, 3):  -0.0400   (2, 3):  -0.0400   (3, 3):  -0.0400   (4, 3):   1.0000\n",
      "   (1, 2):  -0.0400                      (3, 2):  -0.0400   (4, 2):  -1.0000\n",
      "   (1, 1):  -0.0400   (2, 1):  -0.0400   (3, 1):  -0.0400   (4, 1):  -0.0400\n",
      "Utilities:\n",
      "   (1, 3):  -0.0800   (2, 3):  -0.0800   (3, 3):   1.5840   (4, 3):   0.0000\n",
      "   (1, 2):  -0.0800                      (3, 2):  -0.0800   (4, 2):   0.0000\n",
      "   (1, 1):  -0.0800   (2, 1):  -0.0800   (3, 1):  -0.0800   (4, 1):  -0.0800\n",
      "Utilities:\n",
      "   (1, 3):  -0.1200   (2, 3):   1.2112   (3, 3):   1.3232   (4, 3):   0.0000\n",
      "   (1, 2):  -0.1200                      (3, 2):   1.1232   (4, 2):   0.0000\n",
      "   (1, 1):  -0.1200   (2, 1):  -0.1200   (3, 1):  -0.1200   (4, 1):  -0.1200\n",
      "Utilities:\n",
      "   (1, 3):   0.9050   (2, 3):   1.2608   (3, 3):   1.2437   (4, 3):   0.0000\n",
      "   (1, 2):  -0.1600                      (3, 2):   1.0349   (4, 2):   0.0000\n",
      "   (1, 1):  -0.1600   (2, 1):  -0.1600   (3, 1):   0.8346   (4, 1):  -0.1600\n",
      "Utilities:\n",
      "   (1, 3):   1.0431   (2, 3):   1.2071   (3, 3):   1.1965   (4, 3):   0.0000\n",
      "   (1, 2):   0.6520                      (3, 2):   0.9957   (4, 2):   0.0000\n",
      "   (1, 1):  -0.2000   (2, 1):   0.5956   (3, 1):   0.7559   (4, 1):   0.5156\n",
      "Utilities:\n",
      "   (1, 3):   1.0952   (2, 3):   1.1586   (3, 3):   1.1449   (4, 3):   0.0000\n",
      "   (1, 2):   0.9249                      (3, 2):   0.9518   (4, 2):   0.0000\n",
      "   (1, 1):   0.5211   (2, 1):   0.6839   (3, 1):   0.8677   (4, 1):   0.5203\n",
      "Utilities:\n",
      "   (1, 3):   1.0889   (2, 3):   1.1109   (3, 3):   1.0966   (4, 3):   0.0000\n",
      "   (1, 2):   1.0211                      (3, 2):   0.9227   (4, 2):   0.0000\n",
      "   (1, 1):   0.8204   (2, 1):   0.7909   (3, 1):   0.8419   (4, 1):   0.6102\n",
      "Utilities:\n",
      "   (1, 3):   1.0597   (2, 3):   1.0673   (3, 3):   1.0523   (4, 3):   0.0000\n",
      "   (1, 2):   1.0354                      (3, 2):   0.8920   (4, 2):   0.0000\n",
      "   (1, 1):   0.9380   (2, 1):   0.7917   (3, 1):   0.8383   (4, 1):   0.5985\n",
      "Utilities:\n",
      "   (1, 3):   1.0233   (2, 3):   1.0250   (3, 3):   1.0126   (4, 3):   0.0000\n",
      "   (1, 2):   1.0149                      (3, 2):   0.8627   (4, 2):   0.0000\n",
      "   (1, 1):   0.9613   (2, 1):   0.8688   (3, 1):   0.8126   (4, 1):   0.5945\n",
      "Utilities:\n",
      "   (1, 3):   0.9838   (2, 3):   0.9837   (3, 3):   0.9795   (4, 3):   0.0000\n",
      "   (1, 2):   0.9816                      (3, 2):   0.8327   (4, 2):   0.0000\n",
      "   (1, 1):   0.9549   (2, 1):   0.9028   (3, 1):   0.8225   (4, 1):   0.5763\n",
      "Utilities:\n",
      "   (1, 3):   0.9438   (2, 3):   0.9438   (3, 3):   0.9732   (4, 3):   0.0000\n",
      "   (1, 2):   0.9434                      (3, 2):   0.8063   (4, 2):   0.0000\n",
      "   (1, 1):   0.9311   (2, 1):   0.9045   (3, 1):   0.8477   (4, 1):   0.5797\n",
      "Utilities:\n",
      "   (1, 3):   0.9038   (2, 3):   0.9273   (3, 3):   0.9700   (4, 3):   0.0000\n",
      "   (1, 2):   0.9037                      (3, 2):   0.7872   (4, 2):   0.0000\n",
      "   (1, 1):   0.8983   (2, 1):   0.8857   (3, 1):   0.8490   (4, 1):   0.6001\n",
      "Utilities:\n",
      "   (1, 3):   0.8826   (2, 3):   0.9214   (3, 3):   0.9677   (4, 3):   0.0000\n",
      "   (1, 2):   0.8638                      (3, 2):   0.7716   (4, 2):   0.0000\n",
      "   (1, 1):   0.8614   (2, 1):   0.8558   (3, 1):   0.8322   (4, 1):   0.6032\n",
      "Utilities:\n",
      "   (1, 3):   0.8718   (2, 3):   0.9185   (3, 3):   0.9659   (4, 3):   0.0000\n",
      "   (1, 2):   0.8389                      (3, 2):   0.7573   (4, 2):   0.0000\n",
      "   (1, 1):   0.8228   (2, 1):   0.8203   (3, 1):   0.8050   (4, 1):   0.5901\n",
      "Utilities:\n",
      "   (1, 3):   0.8658   (2, 3):   0.9164   (3, 3):   0.9643   (4, 3):   0.0000\n",
      "   (1, 2):   0.8252                      (3, 2):   0.7429   (4, 2):   0.0000\n",
      "   (1, 1):   0.7954   (2, 1):   0.7823   (3, 1):   0.7724   (4, 1):   0.5716\n",
      "Utilities:\n",
      "   (1, 3):   0.8623   (2, 3):   0.9147   (3, 3):   0.9627   (4, 3):   0.0000\n",
      "   (1, 2):   0.8177                      (3, 2):   0.7280   (4, 2):   0.0000\n",
      "   (1, 1):   0.7779   (2, 1):   0.7528   (3, 1):   0.7373   (4, 1):   0.5517\n",
      "Utilities:\n",
      "   (1, 3):   0.8598   (2, 3):   0.9131   (3, 3):   0.9611   (4, 3):   0.0000\n",
      "   (1, 2):   0.8133                      (3, 2):   0.7124   (4, 2):   0.0000\n",
      "   (1, 1):   0.7672   (2, 1):   0.7329   (3, 1):   0.7087   (4, 1):   0.5302\n",
      "Utilities:\n",
      "   (1, 3):   0.8578   (2, 3):   0.9115   (3, 3):   0.9593   (4, 3):   0.0000\n",
      "   (1, 2):   0.8105                      (3, 2):   0.7041   (4, 2):   0.0000\n",
      "   (1, 1):   0.7607   (2, 1):   0.7204   (3, 1):   0.6884   (4, 1):   0.5081\n",
      "Utilities:\n",
      "   (1, 3):   0.8560   (2, 3):   0.9098   (3, 3):   0.9583   (4, 3):   0.0000\n",
      "   (1, 2):   0.8084                      (3, 2):   0.7019   (4, 2):   0.0000\n",
      "   (1, 1):   0.7565   (2, 1):   0.7126   (3, 1):   0.6755   (4, 1):   0.4861\n",
      "Utilities:\n",
      "   (1, 3):   0.8543   (2, 3):   0.9086   (3, 3):   0.9580   (4, 3):   0.0000\n",
      "   (1, 2):   0.8065                      (3, 2):   0.7009   (4, 2):   0.0000\n",
      "   (1, 1):   0.7536   (2, 1):   0.7077   (3, 1):   0.6678   (4, 1):   0.4651\n",
      "Utilities:\n",
      "   (1, 3):   0.8530   (2, 3):   0.9081   (3, 3):   0.9579   (4, 3):   0.0000\n",
      "   (1, 2):   0.8047                      (3, 2):   0.7005   (4, 2):   0.0000\n",
      "   (1, 1):   0.7513   (2, 1):   0.7044   (3, 1):   0.6631   (4, 1):   0.4453\n",
      "Utilities:\n",
      "   (1, 3):   0.8523   (2, 3):   0.9079   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8033                      (3, 2):   0.7004   (4, 2):   0.0000\n",
      "   (1, 1):   0.7493   (2, 1):   0.7019   (3, 1):   0.6599   (4, 1):   0.4390\n",
      "Utilities:\n",
      "   (1, 3):   0.8519   (2, 3):   0.9079   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8025                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7478   (2, 1):   0.6999   (3, 1):   0.6576   (4, 1):   0.4358\n",
      "Utilities:\n",
      "   (1, 3):   0.8517   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8020                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7468   (2, 1):   0.6982   (3, 1):   0.6557   (4, 1):   0.4336\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8018                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7461   (2, 1):   0.6970   (3, 1):   0.6542   (4, 1):   0.4319\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8017                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7457   (2, 1):   0.6963   (3, 1):   0.6531   (4, 1):   0.4305\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7455   (2, 1):   0.6959   (3, 1):   0.6524   (4, 1):   0.4295\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7454   (2, 1):   0.6956   (3, 1):   0.6520   (4, 1):   0.4289\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7454   (2, 1):   0.6955   (3, 1):   0.6517   (4, 1):   0.4284\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6954   (3, 1):   0.6516   (4, 1):   0.4282\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6953   (3, 1):   0.6515   (4, 1):   0.4281\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6953   (3, 1):   0.6515   (4, 1):   0.4280\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6953   (3, 1):   0.6514   (4, 1):   0.4280\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6953   (3, 1):   0.6514   (4, 1):   0.4279\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6953   (3, 1):   0.6514   (4, 1):   0.4279\n",
      "Utilities:\n",
      "   (1, 3):   0.8516   (2, 3):   0.9078   (3, 3):   0.9578   (4, 3):   0.0000\n",
      "   (1, 2):   0.8016                      (3, 2):   0.7003   (4, 2):   0.0000\n",
      "   (1, 1):   0.7453   (2, 1):   0.6953   (3, 1):   0.6514   (4, 1):   0.4279\n",
      "Policy:\n",
      "   (1, 3): Action.Right   (2, 3): Action.Right   (3, 3): Action.Right                       \n",
      "   (1, 2): Action.Up                             (3, 2): Action.Up                          \n",
      "   (1, 1): Action.Up      (2, 1): Action.Left    (3, 1): Action.Left    (4, 1): Action.Left \n"
     ]
    }
   ],
   "source": [
    "def get_initial_U(mdp):\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[s] = mdp.get_reward(s)\n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    for s_p in mdp.get_possible_states():\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.get_reward(s_p)\n",
    "        Q += P * (R + U[s_p])\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    delta = float('inf')\n",
    "    while delta > error:\n",
    "        U = {}\n",
    "        for s in mdp.get_possible_states():\n",
    "            U[s] = U_p[s]\n",
    "        print_U(U)  # to illustrate the iteration process\n",
    "        delta = 0\n",
    "        for s in mdp.get_possible_states():\n",
    "            max_a = float('-inf')\n",
    "            for a in Action:\n",
    "                q = Q_Value(mdp, s, a, U) \n",
    "                if q > max_a:\n",
    "                    max_a = q\n",
    "            U_p[s] = max_a\n",
    "            if abs(U_p[s] - U[s]) > delta:\n",
    "                delta = abs(U_p[s] - U[s])\n",
    "    return U\n",
    "\n",
    "def print_U(U):\n",
    "    print('Utilities:')\n",
    "    for y in range (3, 0, -1):\n",
    "        for x in range(1, 5):\n",
    "            s = (x, y)\n",
    "            if s in U:\n",
    "                print('   {}: {:8.4f}'.format(s, U[s]), end = '')\n",
    "            else: # preserve alignment\n",
    "                print('                   ', end = '')\n",
    "        print()\n",
    "\n",
    "def print_policy(pi):\n",
    "    print('Policy:')\n",
    "    for y in range (3, 0, -1):\n",
    "        for x in range(1, 5):\n",
    "            s = (x, y)\n",
    "            if s in pi:\n",
    "                print('   {}: {:12}'.format(s, pi[s]), end = '')\n",
    "            else: # preserve alignment\n",
    "                print(' '*23, end = '')\n",
    "        print()\n",
    "\n",
    "mdp = SimpleMDPEnvironment(initial_state=(1, 1), reward_per_step=-0.04)\n",
    "U = ValueIteration(mdp)\n",
    "print_U(U)\n",
    "\n",
    "pi_star = {}\n",
    "for s in mdp.get_possible_states():\n",
    "    if mdp.is_done(s):\n",
    "        continue # policy is not needed in stop states\n",
    "    max_a = float('-inf')\n",
    "    argmax_a = None\n",
    "    for action in Action:\n",
    "        q = Q_Value(mdp, s, action, U) \n",
    "        if q > max_a:\n",
    "            max_a = q\n",
    "            argmax_a = action\n",
    "    pi_star[s] = argmax_a\n",
    "print_policy(pi_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 5000 episodes\n",
      "mean:   0.74, sigma:   0.26\n",
      "\n",
      "ep:  1, total reward:  0.72\n",
      "ep:  2, total reward:  0.80\n",
      "ep:  3, total reward:  0.80\n",
      "ep:  4, total reward:  0.76\n",
      "ep:  5, total reward:  0.68\n",
      "......\n",
      "ep: 4995, total reward:  0.80\n",
      "ep: 4996, total reward:  0.84\n",
      "ep: 4997, total reward:  0.80\n",
      "ep: 4998, total reward:  0.80\n",
      "ep: 4999, total reward:  0.84\n"
     ]
    }
   ],
   "source": [
    "def optimal_policy(state):\n",
    "    return pi_star[state]\n",
    "\n",
    "measure_performance(optimal_policy, nr_episodes = 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solving the Bellman Equation: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy:\n",
      "Policy:\n",
      "   (1, 3): Action.Left    (2, 3): Action.Up      (3, 3): Action.Right                       \n",
      "   (1, 2): Action.Right                          (3, 2): Action.Right                       \n",
      "   (1, 1): Action.Right   (2, 1): Action.Up      (3, 1): Action.Down    (4, 1): Action.Right\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(4, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m mdp \u001b[38;5;241m=\u001b[39m SimpleMDPEnvironment(initial_state\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), reward_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.04\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimal policy:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m pi_star \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[1;34m(mdp)\u001b[0m\n\u001b[0;32m     32\u001b[0m max_a, max_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m Action:\n\u001b[1;32m---> 34\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43mQ_Value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;241m>\u001b[39m max_q:\n\u001b[0;32m     36\u001b[0m         max_a, max_q \u001b[38;5;241m=\u001b[39m a, q\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mQ_Value\u001b[1;34m(mdp, s, a, U)\u001b[0m\n\u001b[0;32m     10\u001b[0m     P \u001b[38;5;241m=\u001b[39m mdp\u001b[38;5;241m.\u001b[39mget_transition_prob(a, s_p, s)\n\u001b[0;32m     11\u001b[0m     R \u001b[38;5;241m=\u001b[39m mdp\u001b[38;5;241m.\u001b[39mget_reward(s_p)\n\u001b[1;32m---> 12\u001b[0m     Q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m P \u001b[38;5;241m*\u001b[39m (R \u001b[38;5;241m+\u001b[39m \u001b[43mU\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms_p\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q\n",
      "\u001b[1;31mKeyError\u001b[0m: (4, 2)"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(pi, U, mdp):\n",
    "    U_i = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        if not mdp.is_done(s):\n",
    "            u = 0\n",
    "            for s_p in mdp.get_possible_states():\n",
    "                if not mdp.is_done(s_p):\n",
    "                    P = mdp.get_transition_prob(pi[s], s_p, s)\n",
    "                    R = mdp.get_reward(s_p)\n",
    "                    u += P * (R + U[s])\n",
    "            U_i[s] = u\n",
    "    return U_i\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    # initialise U(s) (arbitrary value 0) and policy pi (arbitrary action Up)\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[s] = 0\n",
    "    pi = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        if not mdp.is_done(s):\n",
    "            pi[s] = choice([a for a in Action])\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        print_policy(pi) # to visualise the iterations\n",
    "        changed = False\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        for s in mdp.get_possible_states():\n",
    "            if not mdp.is_done(s):\n",
    "                # determine action a that yields the highest Q-value\n",
    "                max_a, max_q = None, float('-inf')\n",
    "                for a in Action:\n",
    "                    q = Q_Value(mdp, s, a, U) \n",
    "                    if q > max_q:\n",
    "                        max_a, max_q = a, q\n",
    "                if max_q > Q_Value(mdp, s, pi[s], U):\n",
    "                    pi[s], changed = max_a, True\n",
    "    return pi\n",
    "\n",
    "mdp = SimpleMDPEnvironment(initial_state=(1, 1), reward_per_step=-0.04)\n",
    "print('optimal policy:')\n",
    "pi_star = policy_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
