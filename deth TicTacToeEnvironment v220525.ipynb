{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Agent and Environment\n",
    "\n",
    "This example of a rational agent enables experimentation with decision strategies based on expected utility. \n",
    "\n",
    "The environment interface is inspired by the Open AI gym framework. \n",
    "\n",
    "The example from chapter 16 of AIMA (4th ed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Definition of the Environment\n",
    "\n",
    "The code below defines all characteristics of a Markov Decision Process that models the game of TicTacToe with an (unknown) opponent. The agent plays with X and the opponent plays with O.\n",
    "\n",
    "- States: The Environment is in one of the following states $S = (S_0, S_1, ..., S_8)$ with $S_i \\in \\{E, O, X\\}$ .\n",
    "- Actions: The set of available actions is $a \\in \\{n \\in \\mathbb{N}: S_n = E\\}$ (the agent can place the X on any empty place)\n",
    "- Transitions: The transition depends on the strategy of the opponent. Let's assume the opponent plays in a random fashion. Then the probability that the next O will be on any empty square is equal to 1 / (# free squares left for the opponent)\n",
    "\n",
    "An TicTacToeMDPEnvironment object has the following methods:\n",
    "- reset() which brings the environment the start state, which is also returned\n",
    "- step(action) processes the action of the agent and returns the new state, done, reward (and optional debug info)\n",
    "- render() simple visualisation of the current state of the world\n",
    "\n",
    "To allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "\n",
    "- get_possible_states() for iterating over all possible states\n",
    "- is_done(state) for excluding the stop states from the policy\n",
    "- get_reward(state) simplified version $R(s)$ of the general reward function: $R(s, a, s')$\n",
    "- get_transition_prob(action, new_state, old_state): $P(s' \\mid s, a)$\n",
    "\n",
    "We will illustrate each of the elements above by simple code examples below.  \n",
    "All of the theory can be found in AIMA 16.1 and 16.2.1 (4th ed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from random import randint, choice\n",
    "from copy import copy\n",
    "\n",
    "E, X, O = ' ', 'X', 'O'\n",
    "\n",
    "class TicTacToeMDPEnvironment():\n",
    "    def __init__(self, initial_state=None):\n",
    "        if initial_state == None:\n",
    "            self.__initial_state = ([], [n for n in range(9)]) # start with empty board and 9 possible actions\n",
    "        else:\n",
    "            self.__initial_state = copy(initial_state)  # copy to prevent aliassing\n",
    "        self.__state = self.__initial_state\n",
    "        self.__possible_states = []\n",
    "        self.__calculate_possible_states(self.__initial_state)\n",
    "        \n",
    "    def __calculate_possible_states(self, state):\n",
    "        possible_actions = self.__state[1]\n",
    "        for action in possible_actions:\n",
    "            new_state = copy(state)\n",
    "            if state.count(X) == state.count(O):\n",
    "                new_state[action] = X \n",
    "            else: \n",
    "                new_state[action] = O\n",
    "            self.__possible_states.append(new_state)\n",
    "            if not self.is_done(new_state):\n",
    "                self.__calculate_possible_states(new_state)\n",
    "\n",
    "    def reset(self):\n",
    "        self.__state = self.__initial_state\n",
    "        return self.__state\n",
    "    \n",
    "    def __calculate_transition(self, action):\n",
    "        # change the state to reflect the move by the agent\n",
    "        self.__state[0].append(action)\n",
    "        self.__state[1].remove(action)\n",
    "        if self.is_done(): # check if agent has won\n",
    "            return self.__state\n",
    "        # let the opponent make a random move\n",
    "        opponent_action = choice(self.state[1])\n",
    "        self.__state.append(opponent_action)\n",
    "        self.__state.remove(opponent_action)\n",
    "        return self.__state\n",
    "      \n",
    "    def step(self, action):\n",
    "        old_state = self.__state\n",
    "        self.__state = self.__calculate_transition(action)  # state after action\n",
    "        observation = self.__state  # environment is fully observable\n",
    "        done = self.is_done()\n",
    "        reward = self.get_reward(self.__state)\n",
    "        info = {}  # optional debug info\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    def render(self):\n",
    "        BACKGROUND = [\n",
    "            '   │   │   ',\n",
    "            '───┼───┼───',\n",
    "            '   │   │   ',\n",
    "            '───┼───┼───',\n",
    "            '   │   │   '\n",
    "        ]\n",
    "        rendering = copy(BACKGROUND)\n",
    "        for move_nr, position in enumerate(self.__state[0]):\n",
    "            row = 2 * (position // 3)\n",
    "            col = 4 * (position % 3) + 1\n",
    "            line = rendering[row]\n",
    "            if move_nr % 2 == 0:\n",
    "                rendering[row] = line[:col] + X + line[col + 1:]\n",
    "            else:\n",
    "                rendering[row] = line[:col] + O + line[col + 1:]\n",
    "\n",
    "        for line in rendering:\n",
    "            print(line)\n",
    "        \n",
    "    #=========================================================\n",
    "    # public functions for agent to calculate optimal policy\n",
    "    #=========================================================\n",
    "    \n",
    "    def get_possible_states(self):\n",
    "        return self.__possible_states\n",
    "    \n",
    "    def get_possible_actions(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        return [n for n in range(9) if state[n] == E]\n",
    "\n",
    "    def is_done(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        if state[1] == []:\n",
    "            return True\n",
    "        if state[0] == state[1] == state[2] != E:\n",
    "            return True\n",
    "        if state[3] == state[4] == state[5] != E:\n",
    "            return True\n",
    "        if state[6] == state[7] == state[8] != E:\n",
    "            return True\n",
    "        if state[0] == state[3] == state[6] != E:\n",
    "            return True\n",
    "        if state[1] == state[4] == state[7] != E:\n",
    "            return True\n",
    "        if state[2] == state[5] == state[8] != E:\n",
    "            return True\n",
    "        if state[0] == state[4] == state[8] != E:\n",
    "            return True\n",
    "        if state[2] == state[4] == state[6] != E:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        # Reward R(s) for every possible state\n",
    "        if state[0] == state[1] == state[2] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[3] == state[4] == state[5] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[6] == state[7] == state[8] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[0] == state[3] == state[6] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[1] == state[4] == state[7] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[2] == state[5] == state[8] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[0] == state[4] == state[8] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        if state[2] == state[4] == state[6] != E:\n",
    "            return 1 if state[0] == X else -1\n",
    "        return 0\n",
    "        \n",
    "    def get_transition_prob(self, action, new_state, old_state=None):\n",
    "        if old_state is None:\n",
    "            old_state = self.__state\n",
    "        # returns the Transition Probability P(s'| s, a)\n",
    "        # with s = old_state, a = action and s' = new_state\n",
    "\n",
    "        # if the game is over, no transition can take place\n",
    "        if self.is_done(old_state):\n",
    "            return 0.0\n",
    "        \n",
    "        # the position of the action must be empty\n",
    "        if old_state[action] != E:\n",
    "            return 0.0\n",
    "        \n",
    "        # state after placing X\n",
    "        state_after_X = copy(old_state)  # avoid unwanted changed by reference\n",
    "        state_after_X[action] = X\n",
    "\n",
    "        # check if game is done\n",
    "        if self.is_done(state_after_X) and state_after_X == new_state:\n",
    "            return 1.0\n",
    "\n",
    "        # game is not done: calculate all possible states of the opponent\n",
    "        possible_new_states = []\n",
    "        possible_opponent_actions = self.get_possible_actions(state_after_X)\n",
    "        for action in possible_opponent_actions:\n",
    "            possible_new_state = copy(state_after_X)\n",
    "            possible_new_state[action] = O\n",
    "            possible_new_states.append(possible_new_state)\n",
    "        if new_state not in possible_new_states:\n",
    "            return 0.0\n",
    "        \n",
    "        # transition is possible, apply strategy:\n",
    "        # random opponent, probability is 1 / (# of E before placing the new O)\n",
    "        prob = 1 / (len(possible_new_states))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of an Environment\n",
    "\n",
    "The Environment Class allows creation of an Environment with an initial state as parameter s = (1, 1).\n",
    "Also, method reset() will set the state back to (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# example of creation of an environment in the default state\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mdp \u001b[38;5;241m=\u001b[39m \u001b[43mTicTacToeMDPEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m mdp\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      4\u001b[0m mdp\u001b[38;5;241m.\u001b[39mrender()\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36mTicTacToeMDPEnvironment.__init__\u001b[1;34m(self, initial_state)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__initial_state\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__possible_states \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__calculate_possible_states\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__initial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36mTicTacToeMDPEnvironment.__calculate_possible_states\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     20\u001b[0m new_state \u001b[38;5;241m=\u001b[39m copy(state)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(X) \u001b[38;5;241m==\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(O):\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mnew_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m X \n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m     24\u001b[0m     new_state[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m O\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in the default state\n",
    "mdp = TicTacToeMDPEnvironment()\n",
    "mdp.reset()\n",
    "mdp.render()\n",
    "state, done, reward, info = mdp.step(4)\n",
    "print('state =', state, ', reward =', reward, ', done =', done)\n",
    "mdp.render()\n",
    "print('possible (internal) game states:')\n",
    "mdp.get_possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space and Transitions\n",
    "\n",
    "We will only deal with environments with a finite number of discrete actions.\n",
    "\n",
    "The Action Space (set of all possible actions) can be gotten from the environment.\n",
    "\n",
    "Transitions can be done by calling method step(action).\n",
    "\n",
    "Here is an experiment with a random move by the agent to show the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m mdp \u001b[38;5;241m=\u001b[39m TicTacToeMDPEnvironment([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m                                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m possible_actions \u001b[38;5;241m=\u001b[39m mdp\u001b[38;5;241m.\u001b[39mget_possible_actions()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpossible actions: \u001b[39m\u001b[38;5;124m'\u001b[39m, possible_actions)\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mTicTacToeMDPEnvironment.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m rendering \u001b[38;5;241m=\u001b[39m copy(BACKGROUND)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m move_nr, position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__state):\n\u001b[1;32m---> 64\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mposition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m)\n\u001b[0;32m     65\u001b[0m     col \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m (position \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m     line \u001b[38;5;241m=\u001b[39m rendering[row]\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "mdp = TicTacToeMDPEnvironment(['X', ' ', ' ', \n",
    "                               'O', 'X', ' ', \n",
    "                               ' ', ' ', 'O'])\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('possible actions: ', possible_actions)\n",
    "random_agent_action = choice(possible_actions)\n",
    "new_state, done, reward, info = mdp.step(random_agent_action)\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('possible actions: ', possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition probability $P(s' \\mid s, a)$ can also be returned directly via method get_transition_prob(action, new_state, old_state).  \n",
    "This means that the agent has information about the environment model. N.B. this is not always the case in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m S_4 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     18\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     19\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m S_5 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     22\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     23\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 25\u001b[0m mdp \u001b[38;5;241m=\u001b[39m \u001b[43mTicTacToeMDPEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m mdp\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpossible actions:\u001b[39m\u001b[38;5;124m'\u001b[39m, mdp\u001b[38;5;241m.\u001b[39mget_possible_actions())\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mTicTacToeMDPEnvironment.__init__\u001b[1;34m(self, initial_state)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__initial_state\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__possible_states \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__calculate_possible_states\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__initial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mTicTacToeMDPEnvironment.__calculate_possible_states\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     20\u001b[0m new_state \u001b[38;5;241m=\u001b[39m copy(state)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mcount(X) \u001b[38;5;241m==\u001b[39m state\u001b[38;5;241m.\u001b[39mcount(O):\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mnew_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m X \n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m     24\u001b[0m     new_state[action] \u001b[38;5;241m=\u001b[39m O\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "S_0 = ['X', ' ', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_1 = ['X', 'X', 'O', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_2 = ['X', 'X', ' ', \n",
    "       'O', 'X', 'O', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_3 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       'O', ' ', 'O']\n",
    "\n",
    "S_4 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', 'O', 'O']\n",
    "\n",
    "S_5 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       'X', ' ', 'O']\n",
    "\n",
    "mdp = TicTacToeMDPEnvironment(S_0)\n",
    "mdp.render()\n",
    "\n",
    "print('possible actions:', mdp.get_possible_actions())\n",
    "\n",
    "for n, S_p in enumerate([S_1, S_2, S_3, S_4, S_5], 1):\n",
    "    print('S_0 -> action 1 -> S_' + str(n), 'has probability:', mdp.get_transition_prob(1, new_state=S_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Agent\n",
    "\n",
    "The policy function $\\pi(s) \\to a$ is the concrete implementation of the decision process of the agent (selection of an action $a$). In the cell below, you can see the effect of an agent with a random policy choosing an arbitrary action regardless of the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "action: 6\tstate: [' ', ' ', ' ', ' ', ' ', 'O', 'X', ' ', ' '], reward:  0.00\n",
      "action: 8\tstate: [' ', ' ', ' ', 'O', ' ', 'O', 'X', ' ', 'X'], reward:  0.00\n",
      "action: 7\tstate: [' ', ' ', ' ', 'O', ' ', 'O', 'X', 'X', 'X'], reward: -1.00\n",
      "Episode done after 3 steps. total reward:  -1.00\n",
      "   │   │   \n",
      "───┼───┼───\n",
      " O │   │ O \n",
      "───┼───┼───\n",
      " X │ X │ X \n"
     ]
    }
   ],
   "source": [
    "def policy_random(mdp, state):\n",
    "    # action is random choice from all actions in Action Space\n",
    "    action = choice([a for a in mdp.get_possible_actions(state)])\n",
    "    return action\n",
    "\n",
    "# create a random environment\n",
    "mdp = TicTacToeMDPEnvironment()\n",
    "state = mdp.reset()\n",
    "print('initial state: {}'.format(state))\n",
    "\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "nr_steps = 0\n",
    "while not done:\n",
    "    next_action = policy_random(mdp, state)\n",
    "    state, done, reward, info = mdp.step(next_action)\n",
    "    total_reward += reward\n",
    "    nr_steps += 1\n",
    "    print('action: {}\\tstate: {}, reward: {:5.2f}'\n",
    "          .format(next_action, state, reward))\n",
    "print('Episode done after {} steps. total reward: {:6.2f}'.format(nr_steps, total_reward))\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run from start state until stop state is called an episode.  \n",
    "Let's assemble some statistics on the episodes of the random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 10 episodes\n",
      "[' ', ' ', ' ', 'O', ' ', ' ', 'X', ' ', ' '] False 0 {}\n",
      "[' ', ' ', ' ', 'O', 'X', ' ', 'X', 'O', ' '] False 0 {}\n",
      "['X', 'O', ' ', 'O', 'X', ' ', 'X', 'O', ' '] False 0 {}\n",
      "['X', 'O', 'O', 'O', 'X', 'X', 'X', 'O', ' '] False 0 {}\n",
      "['X', 'O', 'O', 'O', 'X', 'X', 'X', 'O', 'X'] True 1 {}\n",
      "episode: 1 reward: 1.0\n",
      "[' ', ' ', 'X', ' ', ' ', ' ', 'O', ' ', ' '] False 0 {}\n",
      "[' ', 'O', 'X', ' ', 'X', ' ', 'O', ' ', ' '] False 0 {}\n",
      "[' ', 'O', 'X', ' ', 'X', 'O', 'O', ' ', 'X'] False 0 {}\n",
      "['X', 'O', 'X', ' ', 'X', 'O', 'O', ' ', 'X'] True 1 {}\n",
      "episode: 2 reward: 1.0\n",
      "[' ', ' ', ' ', ' ', 'O', ' ', ' ', 'X', ' '] False 0 {}\n",
      "[' ', ' ', ' ', 'O', 'O', 'X', ' ', 'X', ' '] False 0 {}\n",
      "['X', ' ', ' ', 'O', 'O', 'X', ' ', 'X', 'O'] False 0 {}\n",
      "['X', 'O', ' ', 'O', 'O', 'X', 'X', 'X', 'O'] False 0 {}\n",
      "['X', 'O', 'X', 'O', 'O', 'X', 'X', 'X', 'O'] True 0 {}\n",
      "episode: 3 reward: 0.0\n",
      "[' ', 'O', 'X', ' ', ' ', ' ', ' ', ' ', ' '] False 0 {}\n",
      "[' ', 'O', 'X', ' ', 'O', 'X', ' ', ' ', ' '] False 0 {}\n",
      "['X', 'O', 'X', 'O', 'O', 'X', ' ', ' ', ' '] False 0 {}\n",
      "['X', 'O', 'X', 'O', 'O', 'X', ' ', ' ', 'X'] True 1 {}\n",
      "episode: 4 reward: 1.0\n",
      "['O', ' ', ' ', ' ', 'X', ' ', ' ', ' ', ' '] False 0 {}\n",
      "['O', 'X', ' ', 'O', 'X', ' ', ' ', ' ', ' '] False 0 {}\n",
      "['O', 'X', ' ', 'O', 'X', 'O', ' ', ' ', 'X'] False 0 {}\n",
      "['O', 'X', 'X', 'O', 'X', 'O', 'O', ' ', 'X'] True -1 {}\n",
      "episode: 5 reward: -1.0\n",
      "[' ', ' ', ' ', ' ', 'X', ' ', ' ', ' ', 'O'] False 0 {}\n",
      "[' ', 'O', ' ', ' ', 'X', ' ', ' ', 'X', 'O'] False 0 {}\n",
      "[' ', 'O', ' ', ' ', 'X', 'O', 'X', 'X', 'O'] False 0 {}\n",
      "['O', 'O', ' ', 'X', 'X', 'O', 'X', 'X', 'O'] False 0 {}\n",
      "['O', 'O', 'X', 'X', 'X', 'O', 'X', 'X', 'O'] True -1 {}\n",
      "episode: 6 reward: -1.0\n",
      "[' ', ' ', ' ', 'X', ' ', ' ', 'O', ' ', ' '] False 0 {}\n",
      "['X', ' ', ' ', 'X', ' ', ' ', 'O', ' ', 'O'] False 0 {}\n",
      "['X', ' ', ' ', 'X', 'X', 'O', 'O', ' ', 'O'] False 0 {}\n",
      "['X', 'O', 'X', 'X', 'X', 'O', 'O', ' ', 'O'] False 0 {}\n",
      "['X', 'O', 'X', 'X', 'X', 'O', 'O', 'X', 'O'] True 0 {}\n",
      "episode: 7 reward: 0.0\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', 'X', 'O', ' '] False 0 {}\n",
      "[' ', 'O', ' ', ' ', ' ', ' ', 'X', 'O', 'X'] False 0 {}\n",
      "['X', 'O', 'O', ' ', ' ', ' ', 'X', 'O', 'X'] False 0 {}\n",
      "['X', 'O', 'O', 'O', ' ', 'X', 'X', 'O', 'X'] False 0 {}\n",
      "['X', 'O', 'O', 'O', 'X', 'X', 'X', 'O', 'X'] True 1 {}\n",
      "episode: 8 reward: 1.0\n",
      "[' ', 'X', ' ', ' ', 'O', ' ', ' ', ' ', ' '] False 0 {}\n",
      "[' ', 'X', 'X', 'O', 'O', ' ', ' ', ' ', ' '] False 0 {}\n",
      "[' ', 'X', 'X', 'O', 'O', ' ', ' ', 'O', 'X'] False 0 {}\n",
      "[' ', 'X', 'X', 'O', 'O', 'O', 'X', 'O', 'X'] True -1 {}\n",
      "episode: 9 reward: -1.0\n",
      "[' ', ' ', ' ', 'O', ' ', ' ', ' ', 'X', ' '] False 0 {}\n",
      "['O', ' ', ' ', 'O', ' ', 'X', ' ', 'X', ' '] False 0 {}\n",
      "['O', 'O', ' ', 'O', ' ', 'X', ' ', 'X', 'X'] False 0 {}\n",
      "['O', 'O', ' ', 'O', ' ', 'X', 'X', 'X', 'X'] True -1 {}\n",
      "episode: 10 reward: -1.0\n",
      "mean:   0.00, sigma:   0.94\n",
      "\n",
      "ep:  1, total reward:  1.00\n",
      "ep:  2, total reward:  1.00\n",
      "ep:  3, total reward:  0.00\n",
      "ep:  4, total reward:  1.00\n",
      "ep:  5, total reward: -1.00\n",
      "......\n",
      "ep:  5, total reward: -1.00\n",
      "ep:  6, total reward:  0.00\n",
      "ep:  7, total reward:  1.00\n",
      "ep:  8, total reward: -1.00\n",
      "ep:  9, total reward: -1.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def run_one_episode(policy):\n",
    "    mdp = TicTacToeMDPEnvironment()\n",
    "    state = mdp.reset()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_action = policy(mdp, state)\n",
    "        state, done, reward, info = mdp.step(next_action)\n",
    "        print(state, done, reward, info)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def measure_performance(policy, nr_episodes=10):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for n in range(1, N+1):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        print('episode:', n, 'reward:', episode_reward)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print('mean: {:6.2f}, sigma: {:6.2f}'.format(mean(all_rewards), stdev(all_rewards)))\n",
    "    print()\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "    print('......')\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards)-5):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "\n",
    "measure_performance(policy_random)  # in Python a function pointer is simply the name of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above multiple times gives an idea of the typical performance (total reward) of the random policy on this environment. We get a consistent result if we average over enough episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimal decisions based on sums of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman showed in 1957 that the optimal policy $\\pi^{*}(s)$ for an MDP is:\n",
    "\n",
    "(1) $\\pi^{*}(s) = \\underset{a}{argmax} \\space \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$,\n",
    "\n",
    "provided that utility function U(s) satisfies Bellman's equation:\n",
    "\n",
    "(2) $U(s) = \\underset{a}{max} \\space \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$.\n",
    "\n",
    "One can show that Bellman's equation can always be solved and with a single solution.\n",
    "\n",
    "It is useful to define the so-called Q-function:\n",
    "\n",
    "(3) $Q(s, a) = \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$\n",
    "\n",
    "Which simplifies equations (1) and (2) to:\n",
    "\n",
    "(4) $\\pi^{*}(s) = \\underset{a}{argmax} \\space Q(s, a)$  \n",
    "and\n",
    "\n",
    "(5) $U(s) = \\underset{a}{max} \\space Q(s, a)$\n",
    "\n",
    "Thus, finding the optimal policy is reduced to solving Bellman's equation. There are several strategies for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solving the Bellman Equation: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is based on the Bellman update:\n",
    "\n",
    "(6) $U_{i+1}(s) = \\underset{a}{max} \\sum_{s'} P(s' \\mid s, a) \\space [ R(s, a, s') + \\gamma \\space U_i(s') ]$\n",
    "\n",
    "Using equation (3) this simplifies to:\n",
    "\n",
    "(7) $U_{i+1}(s) = \\underset{a}{max} \\space Q_i(s, a)$\n",
    "\n",
    "One can prove that after enough iterations $U_{i+1}(s) \\approx U(s)$, after which Bellman's equation is satisfied.  \n",
    "Since there is only one solution to Bellman's equation, it does not matter with which $U_0(s)$ you start!\n",
    "\n",
    "The algorithm below is Value Iteration with one simplification: $\\gamma$ the so-called discount factor, is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TicTacToeMDPEnvironment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m23\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 58\u001b[0m mdp \u001b[38;5;241m=\u001b[39m \u001b[43mTicTacToeMDPEnvironment\u001b[49m()\n\u001b[1;32m     59\u001b[0m U \u001b[38;5;241m=\u001b[39m ValueIteration(mdp)\n\u001b[1;32m     61\u001b[0m pi_star \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TicTacToeMDPEnvironment' is not defined"
     ]
    }
   ],
   "source": [
    "def get_initial_U(mdp):\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[tuple(s)] = mdp.get_reward(s)\n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    for s_p in mdp.get_possible_states():\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.get_reward(s_p)\n",
    "        Q += P * (R + U[tuple(s_p)])\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    delta = float('inf')\n",
    "    while delta > error:\n",
    "        U = {}\n",
    "        for s in mdp.get_possible_states():\n",
    "            U[tuple(s)] = U_p[tuple(s)]\n",
    "        #print_U(U)  # to illustrate the iteration process\n",
    "        delta = 0\n",
    "        for s in mdp.get_possible_states():\n",
    "            max_a = float('-inf')\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                q = Q_Value(mdp, s, a, U) \n",
    "                if q > max_a:\n",
    "                    max_a = q\n",
    "            U_p[tuple(s)] = max_a\n",
    "            if abs(U_p[tuple(s)] - U[tuple(s)]) > delta:\n",
    "                delta = abs(U_p[tuple(s)] - U[tuple(s)])\n",
    "    return U\n",
    "\n",
    "def print_U(U):\n",
    "    print('Utilities:')\n",
    "    for y in range (3, 0, -1):\n",
    "        for x in range(1, 5):\n",
    "            s = (x, y)\n",
    "            if s in U:\n",
    "                print('   {}: {:8.4f}'.format(s, U[s]), end = '')\n",
    "            else: # preserve alignment\n",
    "                print('                   ', end = '')\n",
    "        print()\n",
    "\n",
    "def print_policy(pi):\n",
    "    print('Policy:')\n",
    "    for y in range (3, 0, -1):\n",
    "        for x in range(1, 5):\n",
    "            s = (x, y)\n",
    "            if s in pi:\n",
    "                print('   {}: {:12}'.format(s, pi[s]), end = '')\n",
    "            else: # preserve alignment\n",
    "                print(' '*23, end = '')\n",
    "        print()\n",
    "\n",
    "mdp = TicTacToeMDPEnvironment()\n",
    "U = ValueIteration(mdp)\n",
    "\n",
    "pi_star = {}\n",
    "for s in mdp.get_possible_states():\n",
    "    if mdp.is_done(s):\n",
    "        continue # policy is not needed in stop states\n",
    "    max_a = float('-inf')\n",
    "    argmax_a = None\n",
    "    for action in Action:\n",
    "        q = Q_Value(mdp, s, action, U) \n",
    "        if q > max_a:\n",
    "            max_a = q\n",
    "            argmax_a = action\n",
    "    pi_star[s] = argmax_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(mdp, state):\n",
    "    return pi_star[state]\n",
    "\n",
    "measure_performance(optimal_policy, nr_episodes = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solving the Bellman Equation: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, U_i, mdp):\n",
    "    U_i = {}\n",
    "    return U\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    # initialise U(s) (arbitrary value 0) and policy pi (arbitrary action Up)\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[s] = 0\n",
    "    pi = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        if not mdp.is_done(s):\n",
    "            pi[s] = choice([a for a in Action])\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        print_policy(pi) # to vilualise the iterations\n",
    "        changed = False\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        for s in mdp.get_possible_states():\n",
    "            if not mdp.is_done(s):\n",
    "                # determine action a that yields the highest Q-value\n",
    "                max_a, max_q = None, float('-inf')\n",
    "                for a in Action:\n",
    "                    q = Q_Value(mdp, s, a, U) \n",
    "                    if q > max_q:\n",
    "                        max_a, max_q = a, q\n",
    "                if max_q > Q_Value(mdp, s, pi[s], U):\n",
    "                    pi[s], changed = max_a, True\n",
    "    return pi\n",
    "\n",
    "mdp = SimpleMDPEnvironment(initial_state=(1, 1), reward_per_step=-0.04)\n",
    "print('optimal policy:')\n",
    "pi_star = policy_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
