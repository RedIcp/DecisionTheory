{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Agent and Environment\n",
    "\n",
    "This example of a rational agent enables experimentation with decision strategies in two ways: \n",
    "1. based on expected utility \n",
    "2. based on rewards obtained after each action\n",
    "\n",
    "The environment is inspired by the Open AI gym framework, but extended for decisions based on expected utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Definition of the Environment\n",
    "\n",
    "The code below defines all characteristics of a simple Vacuum Cleaner Environment with the following characteristics:\n",
    "\n",
    "Environment state:\n",
    "- the agent is in one of two rooms (A or B)\n",
    "- there is a certain amount of dust (d_A) on the floor in room A\n",
    "- there is a certain amount of dust (d_B) on the floor in room B\n",
    "\n",
    "this state is represented by a dictionary.\n",
    "\n",
    "An environment object has the following methods:\n",
    "- reset() which brings the environment in a random (start) state, return value: the state.\n",
    "- step(action) processes the action of the agent and returns the new state, done, reward (and optional debug info)\n",
    "- render() simple visualisation of the current state of the world\n",
    "\n",
    "The actions (Left, Right and Suck) are represented by an enum.\n",
    "\n",
    "We will illustrate each of the elements above by simple code examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from random import random, choice\n",
    "\n",
    "ActionSpace = Enum('ActionSpace', 'Left Right Suck')\n",
    "\n",
    "Room = Enum('Room', 'A B')\n",
    "\n",
    "IN_ROOM_A = [\n",
    "    '┌───────┬───────┐',\n",
    "    '│ A     │ B     │',\n",
    "    '│   ╱╲  │       │',\n",
    "    '│   --  │       │',\n",
    "    '├───────┼───────┤',\n",
    "    '│   0.0 │   0.0 │',\n",
    "    '└───────┴───────┘'\n",
    "]\n",
    "    \n",
    "IN_ROOM_B = [\n",
    "    '┌───────┬───────┐',\n",
    "    '│ A     │ B     │',\n",
    "    '│       │   ╱╲  │',\n",
    "    '│       │   --  │',\n",
    "    '├───────┼───────┤',\n",
    "    '│   0.0 │   0.0 │',\n",
    "    '└───────┴───────┘'\n",
    "]\n",
    "\n",
    "class VacuumCleanerEnvironment():\n",
    "    def __init__(self, room = Room.A, mm_A = 0.5, mm_B = 0.5):\n",
    "        self.__state = {'room': room, 'd_A': mm_A, 'd_B': mm_B}\n",
    "    \n",
    "    def reset(self):\n",
    "        mm_A = round(random(), 1)\n",
    "        mm_B = round(random(), 1)\n",
    "        room = choice([Room.A, Room.B])\n",
    "        self.__state = {'room': room, 'd_A': mm_A, 'd_B': mm_B}\n",
    "        return self.__state\n",
    "        \n",
    "    def get_new_state(self, action):\n",
    "        r, d_A, d_B = self.__state['room'], self.__state['d_A'], self.__state['d_B']\n",
    "        # process the action selected by the agent\n",
    "        if action == ActionSpace.Left:\n",
    "            r = Room.A\n",
    "        elif action == ActionSpace.Right:\n",
    "            r = Room.B\n",
    "        else:  # action == ActionSpace.Suck\n",
    "            if r == Room.A:\n",
    "                d_A = max(0.0, d_A - 0.1)\n",
    "            else:  # self.__state['room'] == Room.B\n",
    "                d_B = max(0.0, d_B - 0.1)\n",
    "        return {'room': r, 'd_A': d_A, 'd_B': d_B}\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.__state = self.get_new_state(action)\n",
    "        observation = self.__state  # state is fully observable\n",
    "        done = self.__state['d_A'] <= 0.001 and self.__state['d_B'] <= 0.001 # prevent rounding errors\n",
    "        reward = -1\n",
    "        info = {}  # optional debug info\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.__state['room'] == Room.A:\n",
    "            rendering = IN_ROOM_A\n",
    "        else:\n",
    "            rendering = IN_ROOM_B\n",
    "        d_A = round(self.__state['d_A'], 1)\n",
    "        d_B = round(self.__state['d_B'], 1)\n",
    "        rendering[5] = '│ ' + str(d_A).rjust(5) + ' │ ' + str(d_B).rjust(5) + ' │'\n",
    "        for line in rendering:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of an Environment\n",
    "\n",
    "The Environment Class allows creation of an Environment in initial state:\n",
    "- default (state parameters not specified at construction)\n",
    "- specific (state parameters specified at construction)\n",
    "- random (by using method reset())\n",
    "\n",
    "Below are examples of all three uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬───────┐\n",
      "│ A     │ B     │\n",
      "│   ╱╲  │       │\n",
      "│   --  │       │\n",
      "├───────┼───────┤\n",
      "│   0.5 │   0.5 │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in the default state\n",
    "env = VacuumCleanerEnvironment()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬───────┐\n",
      "│ A     │ B     │\n",
      "│       │   ╱╲  │\n",
      "│       │   --  │\n",
      "├───────┼───────┤\n",
      "│   0.2 │   0.9 │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in a specific state\n",
    "env = VacuumCleanerEnvironment(Room.B, 0.2, 0.9)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬───────┐\n",
      "│ A     │ B     │\n",
      "│   ╱╲  │       │\n",
      "│   --  │       │\n",
      "├───────┼───────┤\n",
      "│   0.4 │   1.0 │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in a random state\n",
    "env = VacuumCleanerEnvironment()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "We will only deal with environments with a finite number of discrete actions.\n",
    "\n",
    "In that case the so-called Action Space (set of all possible actions) can be easily represented by an enum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 1 is ActionSpace.Left\n",
      "action 2 is ActionSpace.Right\n",
      "action 3 is ActionSpace.Suck\n"
     ]
    }
   ],
   "source": [
    "for nr, action in enumerate(ActionSpace, 1):\n",
    "    print('action', nr, 'is', action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬───────┐\n",
      "│ A     │ B     │\n",
      "│   ╱╲  │       │\n",
      "│   --  │       │\n",
      "├───────┼───────┤\n",
      "│   0.0 │   0.9 │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "env.step(ActionSpace.Suck)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Agent\n",
    "\n",
    "In the cell below, you can see the effect of an agent choosing an arbitrary action regardless of the new state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.4), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.3), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.0, 0.2), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.2), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.2), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.0, 0.2), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.2), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.0, 0.1), reward: -1.0\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.0, 0.0), reward: -1.0\n",
      "\n",
      "episode done. total reward: -32.0\n"
     ]
    }
   ],
   "source": [
    "def select_random_action(state):\n",
    "    # action is random choice from all actions in Action Space\n",
    "    action = choice([a for a in ActionSpace])\n",
    "    return action\n",
    "\n",
    "# create a random environment\n",
    "env = VacuumCleanerEnvironment(Room.A, 0.4, 0.5)\n",
    "state = env.reset()\n",
    "\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "while not done:\n",
    "    next_action = select_random_action(state)\n",
    "    state, done, reward, info = env.step(next_action)\n",
    "    total_reward += reward\n",
    "    print('action: {0}\\tstate: ({1}, {2:.1f}, {3:.1f}), reward: {4:.1f}\\n'\n",
    "          .format(next_action, state['room'], state['d_A'], state['d_B'], reward))\n",
    "print('episode done. total reward:', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell above a couple of times, you can see that the environment always ends up in a state with no dust in rooms A and B, but the agent is not very 'efficient'! It is natural to interpret the negative reward of -1 at each step as a penalty 'wasting time' with 'useless' intermediate steps. Let's see if we can do better using Utility Theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decisions based on Utility\n",
    "\n",
    "Rational Agents have an order of preference for states that is expressed in a Utility Function.\n",
    "\n",
    "As an example: let the Utility of state $s = (r, d_A, d_B)$ be given by:\n",
    "\n",
    "$U(s) = - d_A/2 - d_B$ if $r = RoomA$\n",
    "\n",
    "$U(s) = - d_A - d_B/2$ if $r = RoomB$\n",
    "\n",
    "According to Utility Theory a Ration Agent should choose the action with the highest expected utility (the utility of the new state after the action).\n",
    "\n",
    "In the code example below the agent tries out every action to observe the new state (and calculate the utility). (Note that this neads fresh copy of the environment for every action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: ActionSpace.Suck\tstate: (Room.A, 0.4, 0.5), utility: -0.70\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.4, 0.5), utility: -0.65\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.4, 0.4), utility: -0.60\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.4, 0.3), utility: -0.55\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.4, 0.3), utility: -0.50\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.3, 0.3), utility: -0.45\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.2, 0.3), utility: -0.40\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.2, 0.3), utility: -0.35\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.2, 0.2), utility: -0.30\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.2, 0.1), utility: -0.25\n",
      "\n",
      "action: ActionSpace.Left\tstate: (Room.A, 0.2, 0.1), utility: -0.20\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.1, 0.1), utility: -0.15\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.A, 0.0, 0.1), utility: -0.10\n",
      "\n",
      "action: ActionSpace.Right\tstate: (Room.B, 0.0, 0.1), utility: -0.05\n",
      "\n",
      "action: ActionSpace.Suck\tstate: (Room.B, 0.0, 0.0), utility: -0.00\n",
      "\n",
      "episode done. total reward: -15.0\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy \n",
    "\n",
    "def utility(state):\n",
    "    if state['room'] == Room.A:\n",
    "        return - state['d_A']/2 - state['d_B'] \n",
    "    else:\n",
    "        return - state['d_A'] - state['d_B']/2\n",
    "\n",
    "def select_action_with_max_utility(env, state):\n",
    "    r = state['room']\n",
    "    d_A = state['d_A']\n",
    "    d_B = state['d_B']\n",
    "    max_utility = float('-inf')\n",
    "    best_action = None\n",
    "    for action in ActionSpace:\n",
    "        new_state = env.get_new_state(action)\n",
    "        new_utility = utility(new_state)\n",
    "        if new_utility > max_utility:\n",
    "            best_action = action\n",
    "            max_utility = new_utility\n",
    "    return best_action, max_utility\n",
    "\n",
    "# create a random environment\n",
    "env = VacuumCleanerEnvironment(Room.A, 0.5, 0.5)\n",
    "#state = env.reset()\n",
    "\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "while not done:\n",
    "    best_action, new_utility = select_action_with_max_utility(env, state)\n",
    "    state, done, reward, info = env.step(best_action)\n",
    "    total_reward += reward\n",
    "    print('action: {0}\\tstate: ({1}, {2:.1f}, {3:.1f}), utility: {4:.2f}\\n'\n",
    "          .format(best_action, state['room'], state['d_A'], state['d_B'], new_utility))\n",
    "print('episode done. total reward:', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decisions based on Reward\n",
    "\n",
    "Although decisions based on Utility can lead to efficient behavior, they have drawbacks:\n",
    "- it is not always simple to find a Utility Function that yields all aspects of desired agent behavior\n",
    "- often the utility depends on a sequence of actions rather than a single action, this cannot be handled by the decision process of part 3.\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a. Value Iteration\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b. Policy Iteration\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
