{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - MineSweeper \n",
    "\n",
    "Luis Javier Canto Hurtado\n",
    "\n",
    "Mohammad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TODO Definition of the Environment\n",
    "\n",
    "The code below defines all characteristics of a Markov Decision Process that models the game of Mine Sweeper.\n",
    "\n",
    "- States: The Environment is in one of the following states $S = (S_0, S_1, ..., S_8)$ with $S_i \\in \\{H, B, 0, 1\\}$ H(hidden), B(bomb).\n",
    "- Actions: The set of available actions is $a \\in \\{n \\in \\mathbb{N}: S_n = H\\}$ (the agent can reveal hidden place).\n",
    "- Transitions: #TODO\n",
    "\n",
    "An MineSweeperEnvironment object has the following methods:\n",
    "- reset() which brings the environment the start state, which is also returned\n",
    "- step(action) processes the action of the agent and returns the new state, done, reward (and optional debug info)\n",
    "- render() simple visualisation of the current state of the world\n",
    "\n",
    "To allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "\n",
    "- get_possible_states() for iterating over all possible states\n",
    "- is_done(state) for excluding the stop states from the policy\n",
    "- get_reward(state) simplified version $R(s)$ of the general reward function: $R(s, a, s')$\n",
    "- get_transition_prob(action, new_state, old_state): $P(s' \\mid s, a)$\n",
    "\n",
    "We will illustrate each of the elements above by simple code examples below.  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
=======
   "execution_count": 76,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "from random import randint, choice\n",
    "from copy import copy\n",
    "import itertools\n",
    "    \n",
    "class MineSweeperEnvironment():\n",
    "    def __init__(self, initial_state=None): \n",
    "        if initial_state == None:\n",
    "            self.__initial_state = ['H' for _ in range(9)]\n",
    "        else:\n",
    "            self.__initial_state = copy(initial_state)\n",
    "        self.__state = self.__initial_state\n",
    "        self.__possible_states = []\n",
    "        self.__map = [1, 1, 0, 'B', 1, 0, 1, 1, 0]\n",
    "        self.__calculate_possible_states()\n",
    "    \n",
    "    def __calculate_possible_states(self):\n",
    "        state = copy(self.__state)\n",
    "        self.__possible_states = []\n",
    "        for i in range(len(state)):\n",
    "            temp_state = copy(self.__state)\n",
    "            if state[i] == 'H':\n",
    "                temp_state[i] = self.__map[i]\n",
    "                self.__possible_states.append(temp_state)\n",
    "\n",
    "    def reset(self):\n",
    "        self.__state = self.__initial_state\n",
    "        return self.__state\n",
    "\n",
    "    def __calculate_transition(self, action):\n",
    "        # decrement the value of action by 1 to get the correct index for self.__map\n",
    "        action -= 1\n",
    "        self.__state[action] = self.__map[action]  # reveal the cell and place the value from the initial map\n",
    "        return self.__state\n",
    "\n",
    "    def step(self, action):\n",
    "        old_state = self.__state\n",
    "        self.__state = self.__calculate_transition(action)  # state after action\n",
    "        observation = self.__state  # environment is fully observable\n",
    "        done = self.is_done()\n",
    "        reward = self.get_reward(self.__state)\n",
    "        info = {} # optional debug info\n",
    "        #transition_prob = self.get_transition_prob(old_state, action, self.__state)\n",
    "        #possible_actions = self.get_possible_actions()\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    def render(self):        \n",
    "        BACKGROUND = [\n",
    "            '┌───┬───┬───┐',\n",
    "            '│ H │ H │ H │',\n",
    "            '│───┼───┼───│',\n",
    "            '│ H │ H │ H │',\n",
    "            '│───┼───┼───│',\n",
    "            '│ H │ H │ H │',\n",
    "            '└───┴───┴───┘'\n",
    "        ]\n",
    "        rendering = copy(BACKGROUND)\n",
    "        \n",
<<<<<<< HEAD
    "        mapping = [(1, 2), (1, 6), (1, 10), (3, 2), (3, 6), (3, 10), (5, 2), (5, 6), (5, 10)]\n",
    "        for i, (row, col) in enumerate(mapping):\n",
    "            rendering[row] = rendering[row][:col] + str(self.__state[i]) + rendering[row][col+1:]\n",
    "        \n",
    "        # for i, cell in enumerate(self.__state):\n",
    "        #     row = i // 3\n",
    "        #     col = i % 3\n",
    "        #     if cell != 'H':\n",
    "        #         # replace the current cell value in the rendering list with the cell value from the initial map\n",
    "        #         rendering[2*row+1] = rendering[2*row+1][:3*col+1] + str(self.__map[i]) + rendering[2*row+1][3*col+2:]\n",
=======
    "#         for i, cell in enumerate(self.__state):\n",
    "#             row = i // 3\n",
    "#             col = i % 3\n",
    "#             if cell != 'H':\n",
    "#                 # replace the current cell value in the rendering list with the cell value from the initial map\n",
    "#                 rendering[2*row+1] = rendering[2*row+1][:3*col+1] + str(self.__map[i]) + rendering[2*row+1][3*col+2:]\n",
    "        mapping = [(1, 2), (1, 6), (1, 10), (3, 2), (3, 6), (3, 10), (5, 2), (5, 6), (5, 10)]\n",
    "        for i, (row, col) in enumerate(mapping):\n",
    "            rendering[row] = rendering[row][:col] + str(self.__state[i]) + rendering[row][col+1:]\n",
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
    "        \n",
    "        for line in rendering:\n",
    "            print(line)\n",
    "    \n",
    "    #=========================================================\n",
    "    # public functions for agent to calculate optimal policy\n",
    "    #=========================================================\n",
    "    \n",
    "    def get_possible_states(self):\n",
    "        return self.__possible_states \n",
    "    \n",
    "    def get_possible_actions(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        possible_actions = []\n",
    "        for n, cell in enumerate(self.__state):\n",
    "            # if the element is a hidden cell ('H'), add the index + 1 to the list of possible actions\n",
    "            if cell == 'H':\n",
    "                possible_actions.append(n+1)\n",
    "        return possible_actions\n",
    "\n",
    "    def is_done(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        for cell in state:\n",
    "            if cell == 'B':\n",
    "                return True\n",
    "        state_without_h = [cell for cell in state if cell != 'H']\n",
    "        if len(state_without_h) == len(state) - 1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        state_without_h = [cell for cell in state if cell != 'H']\n",
    "        if 'B' in state_without_h:\n",
    "            return -100  # high penalty for losing the game\n",
    "        elif len(state_without_h) == len(state) - 1:\n",
    "            return 100  # high reward for winning the game\n",
    "        # return a small positive reward for revealing a safe cell\n",
    "        return 1\n",
    "    \n",
    "    # TODO check/fix this funtcion with teacher to make Bellman equation work\n",
    "    def get_transition_prob(self, action, new_state, old_state=None):\n",
    "        if old_state is None:\n",
    "            old_state = self.__state\n",
    "\n",
    "        # transition probability is 0 if the game is over\n",
    "        if self.is_done():\n",
    "            return 0\n",
    "\n",
    "        # transition probability is 0 if action or new_state are not valid\n",
    "        # if action < 0 or action >= len(old_state) or action >= len(new_state):\n",
    "        #     return 0\n",
    "        if old_state[action] != 'H' or new_state[action] == 'H':\n",
    "            return 0\n",
    "\n",
    "        # transition probability is 1 if action and new_state are valid\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of an Environment\n",
    "\n",
    "The Environment Class allows creation of an Environment with an initial state as parameter s = (1, 1).\n",
    "Also, method reset() will set the state back to (1, 1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 73,
=======
   "execution_count": 77,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "└───┴───┴───┘\n",
      "Possible (internal) game states:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'],\n",
       " ['H', 1, 'H', 'H', 'H', 'H', 'H', 'H', 'H'],\n",
       " ['H', 'H', 0, 'H', 'H', 'H', 'H', 'H', 'H'],\n",
       " ['H', 'H', 'H', 'B', 'H', 'H', 'H', 'H', 'H'],\n",
       " ['H', 'H', 'H', 'H', 1, 'H', 'H', 'H', 'H'],\n",
       " ['H', 'H', 'H', 'H', 'H', 0, 'H', 'H', 'H'],\n",
       " ['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 'H'],\n",
       " ['H', 'H', 'H', 'H', 'H', 'H', 'H', 1, 'H'],\n",
       " ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 0]]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 73,
=======
     "execution_count": 77,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp = MineSweeperEnvironment()\n",
    "mdp.reset()\n",
    "mdp.render()\n",
    "print('Possible (internal) game states:')\n",
    "mdp.get_possible_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space and Transitions\n",
    "\n",
    "We will only deal with environments with a finite number of discrete actions.\n",
    "\n",
    "The Action Space (set of all possible actions) can be gotten from the environment.\n",
    "\n",
    "Transitions can be done by calling method step(action).\n",
    "\n",
    "Here is an experiment with a random move by the agent to show the effect."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 74,
=======
   "execution_count": 79,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "└───┴───┴───┘\n",
      "Possible actions:  [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "┌───┬───┬───┐\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
<<<<<<< HEAD
      "│ B │ H │ H │\n",
      "│───┼───┼───│\n",
=======
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ 1 │ H │ H │\n",
      "└───┴───┴───┘\n",
<<<<<<< HEAD
      "Possible actions:  [1, 2, 3, 5, 6, 7, 8, 9]\n"
=======
      "Possible actions:  [1, 2, 3, 4, 5, 6, 8, 9]\n"
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
     ]
    }
   ],
   "source": [
    "mdp = MineSweeperEnvironment()\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('Possible actions: ', possible_actions)\n",
    "random_agent_action = choice(possible_actions)\n",
    "new_state, done, reward, info= mdp.step(random_agent_action)\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('Possible actions: ', possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition probability $P(s' \\mid s, a)$ can also be returned directly via method get_transition_prob(action, new_state, old_state).  \n",
    "This means that the agent has information about the environment model. N.B. this is not always the case in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 75,
=======
   "execution_count": 20,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬───┬───┐\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "└───┴───┴───┘\n",
      "Possible actions: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "S_0 -> action 1 -> S_1 has probability: 0\n",
      "S_0 -> action 1 -> S_2 has probability: 0\n",
      "S_0 -> action 1 -> S_3 has probability: 0\n",
      "S_0 -> action 1 -> S_4 has probability: 1\n",
      "S_0 -> action 1 -> S_5 has probability: 1\n"
     ]
    }
   ],
   "source": [
    "# TODO check with teacher what should be the results and the S_x\n",
    "\n",
    "S_0 = ['H', 'H', 'H', \n",
    "       'H', 'H', 'H', \n",
    "       'H', 'H', 'H']\n",
    "\n",
    "S_1 = [ 1,  'H', 'H', \n",
    "       'H', 'H', 'H', \n",
    "       'H', 'H', 'H']\n",
    "\n",
    "S_2 = [ 1,  'H', 'H', \n",
    "       'B', 'H', 'H', \n",
    "       'H', 'H', 'H']\n",
    "\n",
    "S_3 = [ 1 , 'H', 'H', \n",
    "       'H',  1 , 'H', \n",
    "        1 , 'H', 'H']\n",
    "\n",
    "S_4 = [ 1, 1, 0, \n",
    "       'H',1, 0, \n",
    "        1, 1, 0]\n",
    "\n",
    "S_5 = [ 1, 1 , 0, \n",
    "       'B',1 , 0, \n",
    "        1, 1, 'H']\n",
    "\n",
    "mdp = MineSweeperEnvironment(S_0)\n",
    "mdp.render()\n",
    "\n",
    "print('Possible actions:', mdp.get_possible_actions())\n",
    "\n",
    "for n, S_p in enumerate([S_1, S_2, S_3, S_4, S_5], 1):\n",
    "    print('S_0 -> action 1 -> S_' + str(n), 'has probability:', mdp.get_transition_prob(1, new_state=S_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Agent\n",
    "\n",
    "The policy function $\\pi(s) \\to a$ is the concrete implementation of the decision process of the agent (selection of an action $a$). In the cell below, you can see the effect of an agent with a random policy choosing an arbitrary action regardless of the new state."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 76,
=======
   "execution_count": 21,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
<<<<<<< HEAD
      "action: 7\tstate: ['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 'H'], reward:  1.00\n",
      "action: 5\tstate: ['H', 'H', 'H', 'H', 1, 'H', 1, 'H', 'H'], reward:  1.00\n",
      "action: 6\tstate: ['H', 'H', 'H', 'H', 1, 0, 1, 'H', 'H'], reward:  1.00\n",
      "action: 2\tstate: ['H', 1, 'H', 'H', 1, 0, 1, 'H', 'H'], reward:  1.00\n",
      "action: 8\tstate: ['H', 1, 'H', 'H', 1, 0, 1, 1, 'H'], reward:  1.00\n",
      "action: 1\tstate: [1, 1, 'H', 'H', 1, 0, 1, 1, 'H'], reward:  1.00\n",
      "action: 9\tstate: [1, 1, 'H', 'H', 1, 0, 1, 1, 0], reward:  1.00\n",
      "action: 4\tstate: [1, 1, 'H', 'B', 1, 0, 1, 1, 0], reward: -100.00\n",
      "Episode done after 8 steps. total reward: -93.00\n",
      "┌───┬───┬───┐\n",
      "│ 1 │ 1 │ H │\n",
      "│───┼───┼───│\n",
      "│ B │ 1 │ 0 │\n",
      "│───┼───┼───│\n",
      "│ 1 │ 1 │ 0 │\n",
=======
      "action: 8\tstate: ['H', 'H', 'H', 'H', 'H', 'H', 'H', 1, 'H'], reward:  1.00\n",
      "action: 2\tstate: ['H', 1, 'H', 'H', 'H', 'H', 'H', 1, 'H'], reward:  1.00\n",
      "action: 6\tstate: ['H', 1, 'H', 'H', 'H', 0, 'H', 1, 'H'], reward:  1.00\n",
      "action: 4\tstate: ['H', 1, 'H', 'B', 'H', 0, 'H', 1, 'H'], reward: -100.00\n",
      "Episode done after 4 steps. total reward: -97.00\n",
      "┌───┬───┬───┐\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
      "│───┼───┼───│\n",
      "│ H │ H │ H │\n",
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
      "└───┴───┴───┘\n"
     ]
    }
   ],
   "source": [
    "def policy_random(mdp, state):\n",
    "    action = choice([a for a in mdp.get_possible_actions(state)])\n",
    "    return action\n",
    "\n",
    "mdp = MineSweeperEnvironment()\n",
    "state = mdp.reset()\n",
    "print('Initial state: {}'.format(state))\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "nr_steps = 0\n",
    "\n",
    "while not done:\n",
    "    next_action = policy_random(mdp, state)\n",
    "    state, done, reward, info= mdp.step(next_action)\n",
    "    total_reward += reward\n",
    "    nr_steps += 1\n",
    "    print('action: {}\\tstate: {}, reward: {:5.2f}'.format(next_action, state, reward))\n",
    "print('Episode done after {} steps. total reward: {:6.2f}'.format(nr_steps, total_reward))\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run from start state until stop state is called an episode.  \n",
    "Let's assemble some statistics on the episodes of the random agent:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 77,
=======
   "execution_count": 22,
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 10 episodes\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 'H', 1, 'H'] False 1 {}\n",
<<<<<<< HEAD
      "[1, 'H', 'H', 'H', 'H', 'H', 'H', 1, 'H'] False 1 {}\n",
      "[1, 'H', 0, 'H', 'H', 'H', 'H', 1, 'H'] False 1 {}\n",
      "[1, 'H', 0, 'H', 'H', 0, 'H', 1, 'H'] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 0, 'H', 1, 'H'] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 0, 'H', 1, 0] False 1 {}\n",
      "[1, 1, 0, 'H', 1, 0, 'H', 1, 0] False 1 {}\n",
      "[1, 1, 0, 'B', 1, 0, 'H', 1, 0] True -100 {}\n",
      "episode: 1 reward: -93.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 0, 'H', 'H', 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 0, 'B', 'H', 'H', 1, 'H', 'H'] True -100 {}\n",
      "episode: 2 reward: -98.0\n",
      "['H', 'H', 'H', 'H', 'H', 0, 'H', 'H', 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 0, 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 'H', 'H', 1, 0, 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 0, 'H', 1, 0, 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 0, 'H', 1, 0, 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 0, 'B', 1, 0, 'H', 'H', 'H'] True -100 {}\n",
      "episode: 3 reward: -95.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 0] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 'H', 'H', 'H', 0] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 'H', 1, 'H', 0] False 1 {}\n",
      "[1, 1, 'H', 'H', 'H', 'H', 1, 'H', 0] False 1 {}\n",
      "[1, 1, 'H', 'H', 'H', 'H', 1, 1, 0] False 1 {}\n",
      "[1, 1, 'H', 'H', 1, 'H', 1, 1, 0] False 1 {}\n",
      "[1, 1, 'H', 'B', 1, 'H', 1, 1, 0] True -100 {}\n",
      "episode: 4 reward: -94.0\n",
      "[1, 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 'H', 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 'H', 'H', 1, 'H'] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 0, 'H', 1, 'H'] False 1 {}\n",
      "[1, 1, 0, 'B', 'H', 0, 'H', 1, 'H'] True -100 {}\n",
      "episode: 5 reward: -95.0\n",
      "['H', 'H', 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 0, 'H', 1, 'H', 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 0, 'H', 1, 'H', 'H', 1, 'H'] False 1 {}\n",
      "['H', 1, 0, 'H', 1, 0, 'H', 1, 'H'] False 1 {}\n",
      "['H', 1, 0, 'H', 1, 0, 1, 1, 'H'] False 1 {}\n",
      "['H', 1, 0, 'B', 1, 0, 1, 1, 'H'] True -100 {}\n",
      "episode: 6 reward: -94.0\n",
      "['H', 'H', 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 0, 'H', 1, 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 0, 'H', 1, 'H', 'H', 'H', 0] False 1 {}\n",
      "[1, 'H', 0, 'H', 1, 'H', 'H', 1, 0] False 1 {}\n",
      "[1, 'H', 0, 'H', 1, 'H', 1, 1, 0] False 1 {}\n",
      "[1, 'H', 0, 'B', 1, 'H', 1, 1, 0] True -100 {}\n",
      "episode: 7 reward: -94.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 'H', 1, 1, 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 0, 1, 1, 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "['H', 'H', 0, 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "['H', 1, 0, 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "[1, 1, 0, 'H', 1, 0, 1, 1, 0] True 100 {}\n",
      "episode: 8 reward: 107.0\n",
      "['H', 'H', 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 0, 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 0, 'B', 'H', 'H', 'H', 'H', 'H'] True -100 {}\n",
      "episode: 9 reward: -98.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 0] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 'H', 'H', 'H', 0] False 1 {}\n",
      "[1, 'H', 'H', 'H', 1, 'H', 'H', 'H', 0] False 1 {}\n",
      "[1, 'H', 'H', 'H', 1, 'H', 'H', 1, 0] False 1 {}\n",
      "[1, 'H', 'H', 'B', 1, 'H', 'H', 1, 0] True -100 {}\n",
      "episode: 10 reward: -96.0\n",
      "mean: -75.00, sigma:  63.97\n",
      "\n",
      "ep:  1, total reward: -93.00\n",
      "ep:  2, total reward: -98.00\n",
      "ep:  3, total reward: -95.00\n",
      "ep:  4, total reward: -94.00\n",
      "ep:  5, total reward: -95.00\n",
      "......\n",
      "ep:  5, total reward: -94.00\n",
      "ep:  6, total reward: -94.00\n",
      "ep:  7, total reward: 107.00\n",
      "ep:  8, total reward: -98.00\n",
      "ep:  9, total reward: -96.00\n"
=======
      "['H', 'H', 'H', 'H', 'H', 0, 'H', 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 0, 'H', 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 0, 1, 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 1, 0, 1, 1, 'H'] False 1 {}\n",
      "[1, 1, 'H', 'H', 1, 0, 1, 1, 'H'] False 1 {}\n",
      "[1, 1, 'H', 'B', 1, 0, 1, 1, 'H'] True -100 {}\n",
      "episode: 1 reward: -94.0\n",
      "['H', 'H', 'H', 'B', 'H', 'H', 'H', 'H', 'H'] True -100 {}\n",
      "episode: 2 reward: -100.0\n",
      "['H', 1, 'H', 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 'H', 'H', 1, 'H', 'H', 'H', 'H'] False 1 {}\n",
      "['H', 1, 'H', 'H', 1, 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 1, 'H', 'H', 1, 'H', 1, 1, 'H'] False 1 {}\n",
      "['H', 1, 'H', 'H', 1, 'H', 1, 1, 0] False 1 {}\n",
      "['H', 1, 'H', 'B', 1, 'H', 1, 1, 0] True -100 {}\n",
      "episode: 3 reward: -95.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 0] False 1 {}\n",
      "['H', 'H', 'H', 'B', 'H', 'H', 1, 'H', 0] True -100 {}\n",
      "episode: 4 reward: -98.0\n",
      "['H', 1, 'H', 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 'H', 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 1, 'H', 'H', 'H', 'H', 'H', 'H', 0] False 1 {}\n",
      "[1, 1, 'H', 'H', 'H', 'H', 'H', 1, 0] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 'H', 'H', 1, 0] False 1 {}\n",
      "[1, 1, 0, 'H', 'H', 0, 'H', 1, 0] False 1 {}\n",
      "[1, 1, 0, 'H', 1, 0, 'H', 1, 0] False 1 {}\n",
      "[1, 1, 0, 'B', 1, 0, 'H', 1, 0] True 100 {}\n",
      "episode: 5 reward: 107.0\n",
      "['H', 'H', 'H', 'B', 'H', 'H', 'H', 'H', 'H'] True -100 {}\n",
      "episode: 6 reward: -100.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 'H', 1, 'H', 'H'] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 'H', 1, 'H', 0] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 'H', 1, 1, 0] False 1 {}\n",
      "['H', 'H', 'H', 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "['H', 'H', 0, 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "['H', 'H', 0, 'B', 1, 0, 1, 1, 0] True -100 {}\n",
      "episode: 7 reward: -94.0\n",
      "['H', 'H', 'H', 'H', 'H', 0, 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 0, 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'B', 'H', 0, 'H', 'H', 'H'] True -100 {}\n",
      "episode: 8 reward: -98.0\n",
      "[1, 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 'H', 'H', 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 'H', 1, 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 'H', 0, 1, 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 1, 0, 1, 1, 'H'] False 1 {}\n",
      "[1, 'H', 'H', 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "[1, 'H', 0, 'H', 1, 0, 1, 1, 0] False 1 {}\n",
      "[1, 1, 0, 'H', 1, 0, 1, 1, 0] True 100 {}\n",
      "episode: 9 reward: 107.0\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 0] False 1 {}\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 'H', 0] False 1 {}\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 1, 1, 0] False 1 {}\n",
      "['H', 'H', 'H', 'B', 'H', 'H', 1, 1, 0] True -100 {}\n",
      "episode: 10 reward: -97.0\n",
      "mean: -56.20, sigma:  86.04\n",
      "\n",
      "ep:  1, total reward: -94.00\n",
      "ep:  2, total reward: -100.00\n",
      "ep:  3, total reward: -95.00\n",
      "ep:  4, total reward: -98.00\n",
      "ep:  5, total reward: 107.00\n",
      "......\n",
      "ep:  5, total reward: -100.00\n",
      "ep:  6, total reward: -94.00\n",
      "ep:  7, total reward: -98.00\n",
      "ep:  8, total reward: 107.00\n",
      "ep:  9, total reward: -97.00\n"
>>>>>>> adfbb290ce4532cdd1ea02e117316d1e61794415
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def run_one_episode(policy):\n",
    "    mdp = MineSweeperEnvironment()\n",
    "    state = mdp.reset()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_action = policy(mdp, state)\n",
    "        state, done, reward, info = mdp.step(next_action)\n",
    "        print(state, done, reward, info)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def measure_performance(policy, nr_episodes=10):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for n in range(1, N+1):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        print('episode:', n, 'reward:', episode_reward)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print('mean: {:6.2f}, sigma: {:6.2f}'.format(mean(all_rewards), stdev(all_rewards)))\n",
    "    print()\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "    print('......')\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards)-5):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "\n",
    "measure_performance(policy_random)  # in Python a function pointer is simply the name of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above multiple times gives an idea of the typical performance (total reward) of the random policy on this environment. We get a consistent result if we average over enough episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimal decisions based on sums of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman showed in 1957 that the optimal policy $\\pi^{*}(s)$ for an MDP is:\n",
    "\n",
    "(1) $\\pi^{*}(s) = \\underset{a}{argmax} \\space \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$,\n",
    "\n",
    "provided that utility function U(s) satisfies Bellman's equation:\n",
    "\n",
    "(2) $U(s) = \\underset{a}{max} \\space \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$.\n",
    "\n",
    "One can show that Bellman's equation can always be solved and with a single solution.\n",
    "\n",
    "It is useful to define the so-called Q-function:\n",
    "\n",
    "(3) $Q(s, a) = \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$\n",
    "\n",
    "Which simplifies equations (1) and (2) to:\n",
    "\n",
    "(4) $\\pi^{*}(s) = \\underset{a}{argmax} \\space Q(s, a)$  \n",
    "and\n",
    "\n",
    "(5) $U(s) = \\underset{a}{max} \\space Q(s, a)$\n",
    "\n",
    "Thus, finding the optimal policy is reduced to solving Bellman's equation. There are several strategies for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solving the Bellman Equation: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is based on the Bellman update:\n",
    "\n",
    "(6) $U_{i+1}(s) = \\underset{a}{max} \\sum_{s'} P(s' \\mid s, a) \\space [ R(s, a, s') + \\gamma \\space U_i(s') ]$\n",
    "\n",
    "Using equation (3) this simplifies to:\n",
    "\n",
    "(7) $U_{i+1}(s) = \\underset{a}{max} \\space Q_i(s, a)$\n",
    "\n",
    "One can prove that after enough iterations $U_{i+1}(s) \\approx U(s)$, after which Bellman's equation is satisfied.  \n",
    "Since there is only one solution to Bellman's equation, it does not matter with which $U_0(s)$ you start!\n",
    "\n",
    "The algorithm below is Value Iteration with one simplification: $\\gamma$ the so-called discount factor, is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [80], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[39mprint\u001b[39m()\n\u001b[1;32m     58\u001b[0m mdp \u001b[39m=\u001b[39m MineSweeperEnvironment()\n\u001b[0;32m---> 59\u001b[0m U \u001b[39m=\u001b[39m ValueIteration(mdp)\n\u001b[1;32m     61\u001b[0m pi_star \u001b[39m=\u001b[39m {}\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m mdp\u001b[39m.\u001b[39mget_possible_states():\n",
      "Cell \u001b[0;32mIn [80], line 28\u001b[0m, in \u001b[0;36mValueIteration\u001b[0;34m(mdp, error)\u001b[0m\n\u001b[1;32m     26\u001b[0m max_a \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m mdp\u001b[39m.\u001b[39mget_possible_actions(s):\n\u001b[0;32m---> 28\u001b[0m     q \u001b[39m=\u001b[39m Q_Value(mdp, s, a, U) \n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m q \u001b[39m>\u001b[39m max_a:\n\u001b[1;32m     30\u001b[0m         max_a \u001b[39m=\u001b[39m q\n",
      "Cell \u001b[0;32mIn [80], line 10\u001b[0m, in \u001b[0;36mQ_Value\u001b[0;34m(mdp, s, a, U)\u001b[0m\n\u001b[1;32m      8\u001b[0m Q \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m s_p \u001b[39min\u001b[39;00m mdp\u001b[39m.\u001b[39mget_possible_states():\n\u001b[0;32m---> 10\u001b[0m     P \u001b[39m=\u001b[39m mdp\u001b[39m.\u001b[39;49mget_transition_prob(a, s_p, s)\n\u001b[1;32m     11\u001b[0m     R \u001b[39m=\u001b[39m mdp\u001b[39m.\u001b[39mget_reward(s_p)\n\u001b[1;32m     12\u001b[0m     Q \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m P \u001b[39m*\u001b[39m (R \u001b[39m+\u001b[39m U[\u001b[39mtuple\u001b[39m(s_p)])\n",
      "Cell \u001b[0;32mIn [79], line 123\u001b[0m, in \u001b[0;36mMineSweeperEnvironment.get_transition_prob\u001b[0;34m(self, action, new_state, old_state)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39m# transition probability is 0 if action or new_state are not valid\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# if action < 0 or action >= len(old_state) or action >= len(new_state):\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m#     return 0\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[39mif\u001b[39;00m old_state[action] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m new_state[action] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[39m# transition probability is 1 if action and new_state are valid\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def get_initial_U(mdp):\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[tuple(s)] = mdp.get_reward(s)\n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    for s_p in mdp.get_possible_states():\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.get_reward(s_p)\n",
    "        Q += P * (R + U[tuple(s_p)])\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    delta = float('inf')\n",
    "    while delta > error:\n",
    "        U = {}\n",
    "        for s in mdp.get_possible_states():\n",
    "            U[tuple(s)] = U_p[tuple(s)]\n",
    "        #print_U(U)  # to illustrate the iteration process\n",
    "        delta = 0\n",
    "        for s in mdp.get_possible_states():\n",
    "            max_a = float('-inf')\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                q = Q_Value(mdp, s, a, U) \n",
    "                if q > max_a:\n",
    "                    max_a = q\n",
    "            U_p[tuple(s)] = max_a\n",
    "            if abs(U_p[tuple(s)] - U[tuple(s)]) > delta:\n",
    "                delta = abs(U_p[tuple(s)] - U[tuple(s)])\n",
    "    return U\n",
    "\n",
    "def print_U(U):\n",
    "    print('Utilities:')\n",
    "    for y in range (3, 0, -1):\n",
    "        for x in range(1, 5):\n",
    "            s = (x, y)\n",
    "            if s in U:\n",
    "                print('   {}: {:8.4f}'.format(s, U[s]), end = '')\n",
    "            else: # preserve alignment\n",
    "                print('                   ', end = '')\n",
    "        print()\n",
    "\n",
    "def print_policy(pi):\n",
    "    print('Policy:')\n",
    "    for y in range (3, 0, -1):\n",
    "        for x in range(1, 5):\n",
    "            s = (x, y)\n",
    "            if s in pi:\n",
    "                print('   {}: {:12}'.format(s, pi[s]), end = '')\n",
    "            else: # preserve alignment\n",
    "                print(' '*23, end = '')\n",
    "        print()\n",
    "\n",
    "mdp = MineSweeperEnvironment()\n",
    "U = ValueIteration(mdp)\n",
    "\n",
    "pi_star = {}\n",
    "for s in mdp.get_possible_states():\n",
    "    if mdp.is_done(s):\n",
    "        continue # policy is not needed in stop states\n",
    "    max_a = float('-inf')\n",
    "    argmax_a = None\n",
    "    for action in mdp.get_possible_actions(s):\n",
    "        q = Q_Value(mdp, s, action, U) \n",
    "        if q > max_a:\n",
    "            max_a = q\n",
    "            argmax_a = action\n",
    "    pi_star[s] = argmax_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(mdp, state):\n",
    "    return pi_star[state]\n",
    "\n",
    "measure_performance(optimal_policy, nr_episodes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solving the Bellman Equation: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, U_i, mdp):\n",
    "    U_i = {}\n",
    "    return U\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    # initialise U(s) (arbitrary value 0) and policy pi (arbitrary action Up)\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[s] = 0\n",
    "    pi = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        if not mdp.is_done(s):\n",
    "            pi[s] = choice([a for a in mdp.get_possible_actions(s)])\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        print_policy(pi) # to vilualise the iterations\n",
    "        changed = False\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        for s in mdp.get_possible_states():\n",
    "            if not mdp.is_done(s):\n",
    "                # determine action a that yields the highest Q-value\n",
    "                max_a, max_q = None, float('-inf')\n",
    "                for a in mdp.get_possible_actions(s):\n",
    "                    q = Q_Value(mdp, s, a, U) \n",
    "                    if q > max_q:\n",
    "                        max_a, max_q = a, q\n",
    "                if max_q > Q_Value(mdp, s, pi[s], U):\n",
    "                    pi[s], changed = max_a, True\n",
    "    return pi\n",
    "\n",
    "mdp = MineSweeperEnvironment()\n",
    "print('optimal policy:')\n",
    "pi_star = policy_iteration(mdp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
